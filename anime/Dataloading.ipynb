{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp anime.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.10.21)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.10.30-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.24.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.15.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.14)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.7.2)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
      "Installing collected packages: wandb\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.10.21\n",
      "    Uninstalling wandb-0.10.21:\n",
      "      Successfully uninstalled wandb-0.10.21\n",
      "Successfully installed wandb-0.10.30\n"
     ]
    }
   ],
   "source": [
    "#have to update this so often just going to do it every run\n",
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastai2.torch_basics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ff23ec1b279d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_basics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastai2.torch_basics'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.layers import *\n",
    "from fastai2.data.all import *\n",
    "from fastai2.data.block import *\n",
    "from fastai2.optimizer import *\n",
    "from fastai2.learner import *\n",
    "from fastai2.metrics import *\n",
    "from fastai2.callback.all import *\n",
    "from fastai2.vision.all import *\n",
    "from anime.ugatit import *\n",
    "from fastai2.callback.wandb import WandbCallback\n",
    "from anime.kid import *\n",
    "import wandb\n",
    "from fastai2.fp16_utils import convert_module\n",
    "from UGATIT import ILN,adaILN\n",
    "from fastai2.callback.fp16 import _copy_state\n",
    "import gc\n",
    "#from fast.callback.tensorboard import TensorBoardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anime.kid import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True = use GPU\n",
    "defaults.use_cuda=True \n",
    "path=Path(\"/home/fast/.fastai/data/danbooru2018\")\n",
    "meta_path=path/'metadata/2018000000000016'\n",
    "portraits=pd.read_pickle(path/'portraits')\n",
    "selfie_path=Path('/home/fast/.fastai/data/Selfie-dataset')\n",
    "self_txt= selfie_path/'selfie_dataset.txt'\n",
    "self_txt.exists()\n",
    "attrs='image_name popularity partial_faces is_female baby child teenager youth middle_age senior white black asian oval_face round_face heart_face smiling mouth_open frowning wearing_glasses wearing_sunglasses wearing_lipstick tongue_out duck_face black_hair blond_hair brown_hair red_hair curly_hair straight_hair braid_hair showing_cellphone using_earphone using_mirror braces wearing_hat harsh_lighting dim_lighting'.split()\n",
    "img_size=512 #needs to be >96 oherwise loss is nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "wd=0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSelfiePath(df,dir,i):\n",
    "    dft=df.iloc[[i]]\n",
    "    path=dir/((dft['image_name']).values[0]+'.jpg')\n",
    "    return (dft.values,path)\n",
    "def getImagePath(df,dir,i):\n",
    "    dft=df.iloc[[i]]\n",
    "    path=dir/'512px'/(dft['directory']+'/'+dft['file']).values[0]\n",
    "    return (dft.tags.values[0],path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSList():\n",
    "    def __init__(self,name='selfie2anime'):\n",
    "        self.name=name\n",
    "    def __call__(self):\n",
    "        if(self.name=='selfie2anime'):\n",
    "            return self._selfie2anime()\n",
    "        elif(self.name=='_danbooru'):\n",
    "            return self._danbooru()\n",
    "    def _danbooru(self):\n",
    "        def txt_to_path(img,dir=dir,ext='.jpg'):\n",
    "            return dir/(img+ext)\n",
    "        selfies=pd.read_pickle(selfie_path/'selfies')\n",
    "        selfie_imgs=L(list(selfies['image_name'].apply(partial(txt_to_path,dir=selfie_path/'images'))))\n",
    "        f=partial(txt_to_path,dir=path/'512px',ext='')\n",
    "        locs=portraits['directory']+'/'+portraits['file']\n",
    "        anime_imgs=L(list(locs.apply(f)))\n",
    "        return selfie_imgs,anime_imgs\n",
    "    def _selfie2anime(self):\n",
    "        selfie_imgs=L(Path(\"/home/fast/.fastai/data/selfie2anime/trainA\").ls())\n",
    "        anime_imgs=L(Path(\"/home/fast/.fastai/data/selfie2anime/trainB\").ls())\n",
    "        return selfie_imgs,anime_imgs\n",
    "selfie_imgs,anime_imgs=DSList()()\n",
    "len(anime_imgs),len(selfie_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [PILImage.create]\n",
    "ds_img_tfms = [ToTensor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleImage(Tuple):\n",
    "    def toTensor(self):\n",
    "        img1,img2 = self\n",
    "        return torch.cat([img1,img2], dim=2)\n",
    "    def show(self, ctx=None, **kwargs): \n",
    "        return show_image(self.toTensor(),  ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclePair(Transform):\n",
    "    def __init__(self,bItems):\n",
    "        self.small = bItems #(aItems,bItems) if len(aItems)>len(bItems) else (bItems,aItems)\n",
    "        self.assoc = self\n",
    "        \n",
    "    def encodes(self,i):\n",
    "        \"x: tuple of `i`th image and a random image from same or different class; y: True if same class\"\n",
    "        return CycleImage(i, random.choice(self.small)) #CycleImage(self.large[i], random.choice(self.small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensorTuple(TupleTransform):\n",
    "    \"Convert item to appropriate tensor class\"\n",
    "    order = 15\n",
    "@ToTensorTuple\n",
    "def encodes(self, o:CycleImage): return TensorImage(image2byte(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats={'selfie_means':torch.Tensor([143.81911227601947, 119.46716940677527, 113.92494676532168]),\n",
    " 'selfie_stds':torch.Tensor([67.72407215465199, 64.41496156519109, 52.516515323610015]),\n",
    " 'anime_means':torch.Tensor([173.24273681640625, 155.69161987304688, 135.62557983398438]),\n",
    " 'anime_stds':torch.Tensor([51.790767669677734, 48.620208740234375, 48.380496978759766])}\n",
    "def resized_image(fn:Path, sz=128):\n",
    "    x = Image.open(fn).resize((sz,sz))\n",
    "    # Convert image to tenshttp://127.0.0.1:8888/notebooks/anime/Dataloading.ipynb#or for modeling\n",
    "    x = array(x)\n",
    "    #Shouldn't I be normalizing data?!??!?\n",
    "    if x.ndim==3:\n",
    "        x=tensor(x).permute(2,0,1).float()\n",
    "    elif x.ndim==2:\n",
    "        x=tensor(x)[None].repeat(3,1,1).float()\n",
    "    if(fn.parent.name=='trainA'):\n",
    "        return (x-data_stats['selfie_means'][:,None][:,None])/data_stats['selfie_stds'][:,None][:,None]\n",
    "    elif(fn.parent.name=='trainB'):\n",
    "        return (x-data_stats['anime_means'][:,None][:,None])/data_stats['anime_stds'][:,None][:,None]\n",
    "    raise ValueError('The path was not expected '+str(fn))\n",
    "#OpenAndResize = TupleTransform(partial(resized_image,sz=img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resized_image(fn:Path, sz=128):\n",
    "    x = Image.open(fn).resize((sz,sz))\n",
    "    # Convert image to tenshttp://127.0.0.1:8888/notebooks/anime/Dataloading.ipynb#or for modeling\n",
    "    x = array(x)\n",
    "    #Shouldn't I be normalizing data?!??!?\n",
    "    if x.ndim==3:\n",
    "        return tensor(x).permute(2,0,1).float()/255.\n",
    "    elif x.ndim==2:\n",
    "        return tensor(x)[None].repeat(3,1,1).float()/255.\n",
    "OpenAndResize = TupleTransform(partial(resized_image,sz=img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm=[CyclePair(anime_imgs), OpenAndResize, IntToFloatTensor(div=False)]#+[ToTensorTuple(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_float??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Half(Transform):\n",
    "    \"Move batch to `device` (defaults to `default_device()`)\"\n",
    "    def __init__(self,dtype=torch.half):\n",
    "        self.dtype=dtype\n",
    "        super().__init__(split_idx=None, as_item=False)\n",
    "    def encodes(self, b): return to_half(b)\n",
    "    def decodes(self, b): return to_float(b)\n",
    "\n",
    "    _docs=dict(encodes=\"Move batch to `device`\", decodes=\"Return batch to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Half(),\n",
    "after_batch=[aug_transforms(do_flip=True, max_rotate=10., max_zoom=1.1, max_lighting=0.2),Half(),Cuda()]\n",
    "                            \n",
    "                    #Cuda(),#+aug_transforms(do_flip=True, flip_vert=True, max_rotate=10., max_zoom=1.1, max_lighting=0.2,\n",
    "                   #max_warp=0.2, p_affine=0.75, p_lighting=0.75, xtra_tfms=None,\n",
    "                   #size=None, mode='bilinear', pad_mode=PadMode.Reflection)\n",
    "tfm=Pipeline(tm, as_item=False) #+after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopTensor(Transform):\n",
    "    def encodes(self, x): return CycleImage(torch.Tensor([[[0]]]),torch.Tensor([[[0]]])) #needs to match output of network type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm2=Pipeline([NoopTensor()],as_item=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path=selfie_imgs[0].parent\n",
    "val = ['female_10686.jpg',\n",
    " 'female_10367.jpg',\n",
    " 'female_30988.jpg',\n",
    " 'female_8194.jpg',\n",
    " 'female_18724.jpg',\n",
    " 'female_10087.jpg',\n",
    " 'female_27647.jpg',\n",
    " 'female_13409.jpg',\n",
    " 'female_20608.jpg',\n",
    " 'female_26279.jpg',\n",
    " 'female_7103.jpg',\n",
    " 'female_10615.jpg',\n",
    " 'female_25884.jpg',\n",
    " 'female_31235.jpg',\n",
    " 'female_28991.jpg',\n",
    " 'female_9936.jpg',\n",
    " 'female_33285.jpg',\n",
    " 'female_33491.jpg',\n",
    " 'female_1597.jpg',\n",
    " 'female_13893.jpg',\n",
    " 'female_26481.jpg',\n",
    " 'female_32384.jpg',\n",
    " 'female_18827.jpg',\n",
    " 'female_19176.jpg',\n",
    " 'female_6150.jpg',\n",
    " 'female_1487.jpg',\n",
    " 'female_13914.jpg',\n",
    " 'female_29264.jpg',\n",
    " 'female_17571.jpg',\n",
    " 'female_32817.jpg',\n",
    " 'female_7374.jpg',\n",
    " 'female_16798.jpg',\n",
    " 'female_21739.jpg',\n",
    " 'female_22605.jpg',\n",
    " 'female_21305.jpg',\n",
    " 'female_12988.jpg',\n",
    " 'female_4297.jpg',\n",
    " 'female_9025.jpg',\n",
    " 'female_13560.jpg',\n",
    " 'female_31712.jpg',\n",
    " 'female_1177.jpg',\n",
    " 'female_13787.jpg',\n",
    " 'female_11117.jpg',\n",
    " 'female_10664.jpg',\n",
    " 'female_17134.jpg',\n",
    " 'female_16420.jpg',\n",
    " 'female_95.jpg',\n",
    " 'female_15298.jpg',\n",
    " 'female_32776.jpg',\n",
    " 'female_9353.jpg',\n",
    " 'female_22181.jpg',\n",
    " 'female_32015.jpg',\n",
    " 'female_27565.jpg',\n",
    " 'female_19001.jpg',\n",
    " 'female_8955.jpg',\n",
    " 'female_33094.jpg',\n",
    " 'female_7486.jpg',\n",
    " 'female_32167.jpg',\n",
    " 'female_28716.jpg',\n",
    " 'female_6519.jpg',\n",
    " 'female_4339.jpg',\n",
    " 'female_24557.jpg',\n",
    " 'female_20786.jpg',\n",
    " 'female_26695.jpg',\n",
    " 'female_25546.jpg',\n",
    " 'female_15424.jpg',\n",
    " 'female_711.jpg',\n",
    " 'female_31004.jpg',\n",
    " 'female_9440.jpg',\n",
    " 'female_22912.jpg',\n",
    " 'female_2754.jpg',\n",
    " 'female_24839.jpg',\n",
    " 'female_12741.jpg',\n",
    " 'female_9480.jpg',\n",
    " 'female_15677.jpg',\n",
    " 'female_23557.jpg',\n",
    " 'female_3006.jpg',\n",
    " 'female_29858.jpg',\n",
    " 'female_8664.jpg',\n",
    " 'female_16808.jpg',\n",
    " 'female_32694.jpg',\n",
    " 'female_5019.jpg',\n",
    " 'female_15584.jpg',\n",
    " 'female_32256.jpg',\n",
    " 'female_5747.jpg',\n",
    " 'female_22101.jpg',\n",
    " 'female_4115.jpg',\n",
    " 'female_20185.jpg',\n",
    " 'female_25131.jpg',\n",
    " 'female_31061.jpg',\n",
    " 'female_2546.jpg',\n",
    " 'female_30868.jpg',\n",
    " 'female_17341.jpg',\n",
    " 'female_9415.jpg',\n",
    " 'female_14142.jpg',\n",
    " 'female_11470.jpg',\n",
    " 'female_683.jpg',\n",
    " 'female_3433.jpg',\n",
    " 'female_19286.jpg',\n",
    " 'female_30070.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfieL=0\n",
    "selfieL=len(selfie_imgs)-100 if selfieL<3 or selfieL>len(selfie_imgs)-100 else selfieL\n",
    "selfie_sample=selfie_imgs.filter(lambda s:s.name in val,negate=True)\n",
    "selfie_sample=selfie_sample.shuffle()[0:selfieL]\n",
    "selfie_sample=selfie_sample+map(lambda v:val_path/v,val)\n",
    "#val_size=100#20 if selfieL>24 else selfieL//2\n",
    "#val=selfie_sample[0:val_size] set above\n",
    "tfmDs=DataSource(selfie_sample,[tfm,tfm2],splits=[[selfie.name not in val for selfie in selfie_sample],[selfie.name in val for selfie in selfie_sample]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl=TfmdDL(tfmDs,bs=1, shuffle=False, num_workers=8,after_batch=after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=dl.databunch(bs=1,val_bs=1,after_batch=after_batch)\n",
    "# DataBunch(dl) testing above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typedispatch\n",
    "def show_batch(x, y, samples, ctxs=None, max_n=9, **kwargs):\n",
    "    if ctxs is None: ctxs = Inf.nones\n",
    "    for i in range_of(samples[0]):\n",
    "        [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selfie_stds=[selfie[0][0].std() for selfie in iter(dl)]\n",
    "#anime_stds=[anime[0][1].std() for anime in iter(dl)]\n",
    "#sum(selfie_stds)/len(selfie_stds),sum(anime_stds)/len(anime_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb; pdb.set_trace()\n",
    "dl.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CycleImage(dl._pre_show_batch(dl.one_batch(), max_n=9)[0][0][0],dl._pre_show_batch(dl.one_batch(), max_n=9)[0][1][0]).show()\n",
    "CycleImage(dl._pre_show_batch(dl.one_batch(), max_n=9)[1][0][0],dl._pre_show_batch(dl.one_batch(), max_n=9)[1][1][0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(dl._pre_show_batch(dl.one_batch(), max_n=9)[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugatit=UGATIT()\n",
    "ugatit.img_size=img_size\n",
    "ugatit.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.create_opt??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noop_loss = lambda x: torch.FloatTensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANOptimizer(object):\n",
    "    def __init__(self, gen_optim,disc_optim,params,lr,wd=0.01):\n",
    "        gen_params,disc_params=params\n",
    "        #might want to try lower lr for the generator\n",
    "        self.gen_optim,self.disc_optim=gen_optim(gen_params,lr=lr,wd=wd),disc_optim(disc_params,lr=lr,wd=wd)\n",
    "        self.optimizing_gen=True\n",
    "    def __getattr__(self, item):\n",
    "        opt= self.gen_optim if(self.optimizing_gen) else self.disc_optim\n",
    "        return getattr(opt, item)\n",
    "    def zero_grad(self):\n",
    "        self.gen_optim.zero_grad()\n",
    "        self.disc_optim.zero_grad()\n",
    "    def state_dict(self):\n",
    "        gs = self.gen_optim.state_dict()\n",
    "        ds = self.disc_optim.state_dict()\n",
    "        return {'hypers':{\n",
    "                    'gen':gs['hypers'],\n",
    "                    'disc':ds['hypers']},\n",
    "                'state':{\n",
    "                    'gen':gs['state'],\n",
    "                    'disc':ds['state']}}\n",
    "    def load_state_dict(self, sdict):\n",
    "        self.gen_optim.load_state_dict({'hypers': sdict['param_groups']['gen'],'state':sdict['state']['gen']})\n",
    "        self.disc_optim.load_state_dict({'param_groups': sdict['param_groups']['disc'],'state':sdict['state']['disc']})\n",
    "        \n",
    "#TODO unable to run one_batch twice without two after_batch calls\n",
    "#TODO discriminator probably doesn't need gradients from generator, check cycle gan code/paper\n",
    "class Switch(Callback):\n",
    "    toward_end=True\n",
    "    def begin_epoch(self):\n",
    "        self.learn.disc_pass=False #TODO too many (not disc_pass), switch to gen_pass?!?\n",
    "        self.model.optimizing_gen= not self.learn.disc_pass\n",
    "        if(hasattr(self, 'gan_losses')):\n",
    "            self.learn.loss_func=self.gan_losses[self.learn.disc_pass]\n",
    "        self.opt.zero_grad()\n",
    "    def begin_batch(self):\n",
    "        if(not hasattr(self, 'gan_losses')):\n",
    "            self.gan_losses=self.loss_func\n",
    "            self.learn.loss_func=self.gan_losses[self.learn.disc_pass]\n",
    "        self.opt.zero_grad()\n",
    "        self.learn.loss_func=self.gan_losses[self.learn.disc_pass]\n",
    "    def after_batch(self):\n",
    "        if(getattr(self.learn,'overflow',False)):\n",
    "            self.learn.overflow=False\n",
    "            if(self.learn.disc_pass):\n",
    "                self.one_batch(self.iter,self.learn.dbunch.train_dl.one_batch())\n",
    "            return\n",
    "        self._switch()\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        self.loss_func=self.gan_losses\n",
    "    \n",
    "    def _switch(self):\n",
    "        if(self.learn.training): self.learn.disc_pass=not self.learn.disc_pass\n",
    "        self.model.optimizing_gen= not self.learn.disc_pass\n",
    "        self.opt.optimizing_gen= not self.learn.disc_pass\n",
    "        self.opt.zero_grad()\n",
    "        self.learn.loss_func=self.gan_losses[self.learn.disc_pass]\n",
    "        if(self.learn.disc_pass):\n",
    "            self.one_batch(self.iter,L(self.xb+self.yb))\n",
    "        \n",
    "class RhoClipper(Callback):\n",
    "    run_after=Switch\n",
    "    toward_end=True\n",
    "    def after_batch(self):\n",
    "        self.model.models['GA2B'].apply(ugatit.Rho_clipper)\n",
    "        self.model.models['GB2A'].apply(ugatit.Rho_clipper)\n",
    "class PrintLoss(Callback):\n",
    "    def after_loss(self):\n",
    "        if(self.iter%100==0):    \n",
    "            print('batch: #'+str(self.iter)+'/'+str(self.n_iter)+'/'+str(self.epoch))\n",
    "            if(self.learn.disc_pass):\n",
    "                print('loss Disc: '+str(self.loss.item()))\n",
    "            else:\n",
    "                print('loss Gen: '+str(self.loss.item()))\n",
    "    def begin_batch(self):\n",
    "        if(self.iter%100==0):\n",
    "            print('begin batch - batch: #'+str(self.iter)+'/'+str(self.n_iter)+'/'+str(self.epoch))\n",
    "            print('memory_allocated()',torch.cuda.memory_allocated())\n",
    "            print('max_memory_allocated()',torch.cuda.max_memory_allocated())\n",
    "    def after_pred(self):\n",
    "        if(self.iter%100==0):\n",
    "            print('after pred- batch: #'+str(self.iter)+'/'+str(self.n_iter)+'/'+str(self.epoch))\n",
    "            print('memory_allocated()',torch.cuda.memory_allocated())\n",
    "            print('max_memory_allocated()',torch.cuda.max_memory_allocated())\n",
    "    def after_backward(self):\n",
    "        if(self.iter%100==0):\n",
    "            print('batch: #'+str(self.iter)+'/'+str(self.n_iter)+'/'+str(self.epoch))\n",
    "            print('memory_allocated()',torch.cuda.memory_allocated())\n",
    "            print('max_memory_allocated()',torch.cuda.max_memory_allocated())\n",
    "\n",
    "            for obj in gc.get_objects():\n",
    "                try:\n",
    "                    if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                        if obj.device==default_device() and obj.dtype==torch.float:\n",
    "                            print(type(obj), obj.size(), obj.device, obj.dtype)\n",
    "                except:\n",
    "                    pass\n",
    "class LinearDecreaseLR(Callback):\n",
    "    def __init__(self,start_epoch=0,end_epoch=None):\n",
    "        store_attr(self,\"start_epoch,end_epoch\")\n",
    "    def begin_fit(self):\n",
    "        self.max_lr=self.lr\n",
    "    def end_fit(self):\n",
    "        self.lr=self.max_lr\n",
    "    def begin_batch(self):\n",
    "        end_epoch = self.end_epoch if(self.end_epoch) else self.n_epoch\n",
    "        current_iter=(self.epoch+self.start_epoch)*self.n_iter+self.iter\n",
    "        run_length=end_epoch*self.n_iter\n",
    "        half_run=run_length // 2\n",
    "        self.lr=self.max_lr\n",
    "        if current_iter > half_run:\n",
    "            self.lr = (run_length-current_iter)*self.max_lr/(run_length-half_run)\n",
    "        elif current_iter < half_run/2:\n",
    "            self.lr = self.max_lr*(current_iter+1)/(half_run/2+1)\n",
    "        else:\n",
    "            self.lr = self.max_lr\n",
    "        self.opt.set_hyper('lr',self.lr)\n",
    "        \n",
    "class SameXbYb(Callback):\n",
    "    def after_pred(self):\n",
    "        self.learn.yb=self.learn.xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@typedispatch\n",
    "#def tensorboard_log(x:CycleImage, y: CycleImage, samples, outs, writer, step):\n",
    "#    fig,axs = get_grid(len(samples), add_vert=1, return_fig=True, double=True)\n",
    "#    for i in range(2):\n",
    "#        axs[::2] = [b.show(ctx=c) for b,c in zip(samples.itemgot(i),axs[::2])]\n",
    "#    for x in [samples,outs]:\n",
    "#        axs[1::2] = [b.show(ctx=c) for b,c in zip(x.itemgot(0),axs[1::2])]\n",
    "#    writer.add_figure('Sample Results', fig, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typedispatch\n",
    "def wandb_process(x:CycleImage, y:CycleImage, samples, outs):\n",
    "    \"Process `sample` and `out` depending on the type of `x/y`\"\n",
    "    res = []\n",
    "    for s,o in zip(samples, outs):\n",
    "        img = TensorImage(s[0].toTensor()[0])\n",
    "        #db.valid_dl.decode(s)\n",
    "        res.append(wandb.Image(img, caption='Input data', grouping=3))\n",
    "        for t, capt in ((o[0], \"Prediction\"), (s[1], \"Ground Truth\")):\n",
    "            # Resize plot to image resolution (from https://stackoverflow.com/a/13714915)\n",
    "            my_dpi = 100\n",
    "            fig = plt.figure(frameon=False, dpi=my_dpi)\n",
    "            h, w = img.shape[:2]\n",
    "            fig.set_size_inches(w / my_dpi, h / my_dpi)\n",
    "            ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            # Superimpose label or prediction to input image\n",
    "            ax = img.show(ctx=ax)\n",
    "            #import pdb; pdb.set_trace()\n",
    "            ax = t.show(ctx=ax)\n",
    "            res.append(wandb.Image(fig, caption=capt))\n",
    "            plt.close(fig)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logs=Path('/home/fast/.fastai/'+'tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricRecorder(Callback):\n",
    "#    def __init__(self, add_time=True, train_metrics=False, beta=0.98):\n",
    "#        super(MetricRecorder,self).__init__(add_time,train_metrics,beta)\n",
    "    def after_batch(self):\n",
    "        mets = self.learn.recorder._train_mets if self.training else self.learn.recorder._valid_mets\n",
    "        self.learn.metric_values=dict([(met.name, met.value) for met in mets])\n",
    "#    @property\n",
    "#    def name(self):\n",
    "#        return 'recorder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditional_Loss(AvgSmoothLoss):\n",
    "    def __init__(self, f=lambda x:True,name='None',beta=0.98,): \n",
    "        super(Conditional_Loss,self).__init__(beta)\n",
    "        self.f=f\n",
    "        self.metric_name=name\n",
    "    def accumulate(self, learn):\n",
    "        if(self.f(learn)):\n",
    "            super().accumulate(learn)\n",
    "            \n",
    "            setattr(learn, self.metric_name, self.value)\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.metric_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/NVIDIA/apex/blob/03a25ba8a61d15a503405ef4bbda83724bb531b2/apex/fp16_utils/fp16util.py\n",
    "# had to edit due to custom normalization layers\n",
    "def convert_module_alt(module, dtype):\n",
    "    \"\"\"\n",
    "    Converts a module's immediate parameters and buffers to dtype.\n",
    "    \"\"\"\n",
    "    for param in module.parameters(recurse=False):\n",
    "        if param is not None:\n",
    "            if param.data.dtype.is_floating_point:\n",
    "                param.data = param.data.to(dtype=dtype)\n",
    "            if param._grad is not None and param._grad.data.dtype.is_floating_point:\n",
    "                param._grad.data = param._grad.data.to(dtype=dtype)\n",
    "    spectral_norm_buffers=filter(lambda b:b[0] in ('weight_u','weight_v','weight_orig'),module.named_buffers(recurse=False))\n",
    "    spectral_norm_buffers=list(map(lambda b: b[1],spectral_norm_buffers))\n",
    "    spectral_norm_buffers=[]\n",
    "    for buf in module.buffers(recurse=False):\n",
    "        if buf is not None and buf.data.dtype.is_floating_point and 0==len(list(filter(lambda b:b.shape==buf.shape and torch.all(torch.eq(b, buf)),spectral_norm_buffers))):\n",
    "            buf.data = buf.data.to(dtype=dtype)\n",
    "def convert_network(network, dtype):\n",
    "    \"\"\"\n",
    "    Converts a network's parameters and buffers to dtype.\n",
    "    \"\"\"\n",
    "    for module in network.modules():\n",
    "        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm) and module.affine is True:\n",
    "            continue\n",
    "        if isinstance(module, (ILN,adaILN,nn.InstanceNorm2d,nn.LayerNorm)):\n",
    "            continue\n",
    "        convert_module_alt(module, dtype)\n",
    "        if isinstance(module, torch.nn.RNNBase) or isinstance(module, torch.nn.modules.rnn.RNNBase):\n",
    "            module.flatten_parameters()\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_model_params(models):\n",
    "    ps=[]\n",
    "    for m in models:\n",
    "        ps+=[p for p in m.parameters() if p.requires_grad]\n",
    "    return ps\n",
    "def GAN_splitter(model):\n",
    "    gen_params=itertools.chain(model.models['GA2B'].parameters(),model.models['GB2A'].parameters())\n",
    "    disc_params=itertools.chain(model.models['DA'].parameters(),model.models['DB'].parameters(),model.models['LA'].parameters(),model.models['LB'].parameters())\n",
    "    return (gen_params,disc_params)\n",
    "class GANWandb(WandbCallback):\n",
    "    run_before=Switch\n",
    "    def after_batch(self):\n",
    "        \"Log hyper-parameters and training loss\"\n",
    "        g_scale,d_scale = (self.learn.mixed_precision_tuple.gloss_scale,self.learn.mixed_precision_tuple.dloss_scale) if(hasattr(self.learn,'mixed_precision_tuple')) else (1,1)\n",
    "        if self.training and not self.learn.disc_pass:\n",
    "            self._wandb_step += 1\n",
    "            self._wandb_epoch += 1/self.n_iter\n",
    "            hypers = {f'{k}G_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "            wandb.log({'epoch': self._wandb_epoch,'loss': self.smooth_loss, **hypers, **self.learn.metric_values, 'loss_scaleG': g_scale}, step=self._wandb_step)\n",
    "        elif self.training and self.learn.disc_pass:\n",
    "            hypers = {f'{k}D_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "            wandb.log({'epoch': self._wandb_epoch,'loss': self.smooth_loss, **hypers, **self.learn.metric_values, 'loss_scaleD': d_scale}, step=self._wandb_step)\n",
    "#   def after_epoch(self):\n",
    "#        \"Log validation loss and custom metrics & log prediction samples\"\n",
    "#        # Correct any epoch rounding error and overwrite value\n",
    "#        self._wandb_epoch = round(self._wandb_epoch)\n",
    "#        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n",
    "#        # Log sample predictions\n",
    "#        if self.log_preds:\n",
    "#            bs=[]\n",
    "#            outs=[]\n",
    "#            for i in range(int(self.n_preds/self.learn.dbunch.bs)):\n",
    "#                b = self.valid_dl.one_batch()\n",
    "#                self.learn.one_batch(0, b)\n",
    "#                preds = getattr(self.loss_func, 'activation', noop)(self.pred)\n",
    "#                out = getattr(self.loss_func, 'decodes', noop)(preds)\n",
    "#                bs += b.detach().cpu()\n",
    "#                outs += out\n",
    "#            x,y,its,outs = self.valid_dl.show_results(b, out, show=False, max_n=self.n_preds)\n",
    "#            wandb.log({\"Prediction Samples\": wandb_process(x, y, its, outs)}, step=self._wandb_step)\n",
    "#        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}, step=self._wandb_step)       \n",
    " \n",
    "#targs_func = lambda b: np.resize(b[0][1].numpy(),[1,3,299,299])\n",
    "#kid=KIDCallback(Tensor(array(list(map(targs_func,db.valid_dl)))))\n",
    "#metrics=kid.kid\n",
    "gen_loss=Conditional_Loss(lambda learn: not learn.disc_pass, name='Gloss_Metric')\n",
    "disc_loss=Conditional_Loss(lambda learn: learn.disc_pass, name='Dloss_Metric')\n",
    "#cycle_weight = 2.,identity_weight = 2.,cam_weight = 8.,adv_weight=1.\n",
    "model=UgatitModel(ugatit.models)\n",
    "model=convert_network(model, dtype=torch.float16)\n",
    "learner=Learner(db,model,UGATITLoss().losses,partial(GANOptimizer,Adam,Adam,wd=wd),lr,splitter=GAN_splitter,metrics=[gen_loss,disc_loss])\n",
    "learner.add_cbs(L([Switch(),RhoClipper(),PrintLoss(),SameXbYb(),GANWandb(valid_dl=learner.dbunch.valid_dl)]))\n",
    "#LinearDecreaseLR(),\n",
    "#learner.load(path/'30_fp32_90x90_1_15',strict=False)\n",
    "#rec=learner.cbs.filter(lambda f:f.name=='recorder')[0]\n",
    "#learner.remove_cb(rec)\n",
    "learner.add_cb(MetricRecorder())\n",
    "learner.recorder.train_metrics=True\n",
    "#learner.load('fp32_10',with_opt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner=learner.load((Path('/home/fast/.fastai/m1')),device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using just the generator, discriminator, and pre-trained GAN output. Loss function would be adverserial loss + MSE-ish loss with pre-trained GANs output. LR would have to decrease overtime and/or wight between adv loss and MSE loss would need to be more adverserial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_master(opt, flat_master=False, device='cuda'):\n",
    "    model_params = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]\n",
    "    if flat_master:\n",
    "        master_params = []\n",
    "        for pg in model_params:\n",
    "            mp = parameters_to_vector([param.data.float() for param in pg])\n",
    "            mp = nn.Parameter(mp, requires_grad=True)\n",
    "            if mp.grad is None: mp.grad = mp.new(*mp.size())\n",
    "            master_params.append([mp])\n",
    "    else:\n",
    "        master_params = [[nn.Parameter(param.data.to(device=device).float().detach(), requires_grad=True) for param in pg] for pg in model_params]\n",
    "    return model_params, master_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgs_to_device(pgs,device=default_device()):\n",
    "    to_device = lambda p:p.to(device=device)\n",
    "    return [list(map(to_device,pg)) for pg in pgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelToHalfAdaLIN(Callback):\n",
    "    \"Use with MixedPrecision callback (but it needs to run at the very beginning)\"\n",
    "    run_before=TrainEvalCallback\n",
    "    #def begin_fit(self): \n",
    "    #    self.learn.model.to(torch.device('cpu')) #trying to not load multiple models on gpu at once\n",
    "    #    self.learn.model = convert_network(self.model, dtype=torch.float16)\n",
    "    #    self.learn.model.to(default_device())\n",
    "    #def begin_batch(self): self.learn.xb = to_half(self.xb)\n",
    "    def after_fit(self): self.learn.model = convert_network(self.model, dtype=torch.float32)\n",
    "class MixedPrecisionTuple(MixedPrecision):\n",
    "    def __init__(self,multi_loss=False,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multi_loss=multi_loss\n",
    "        self.gloss_scale=self.loss_scale\n",
    "        self.dloss_scale=self.loss_scale\n",
    "    def begin_fit(self):\n",
    "        if self.learn.opt is None: self.learn.create_opt()\n",
    "        self.gen_model_pgs,self.gen_master_pgs = get_master(self.opt.gen_optim, self.flat_master, device='cpu')\n",
    "        self.gen_old_pgs = self.opt.gen_optim.param_groups\n",
    "        #Changes the optimizer so that the optimization step is done in FP32.\n",
    "        _copy_state(self.learn.opt.gen_optim, self.gen_model_pgs, self.gen_master_pgs)\n",
    "        self.disc_model_pgs,self.disc_master_pgs = get_master(self.opt.disc_optim, self.flat_master, device='cpu')\n",
    "        self.disc_old_pgs = self.opt.disc_optim.param_groups\n",
    "        #Changes the optimizer so that the optimization step is done in FP32.\n",
    "        _copy_state(self.learn.opt.disc_optim, self.disc_model_pgs, self.disc_master_pgs)\n",
    "\n",
    "        if self.dynamic: \n",
    "            self.g_count = self.d_count = self.count = 0\n",
    "    def begin_batch(self):\n",
    "        if(self.learn.disc_pass):\n",
    "            #tmp=(pgs_to_device(self.gen_master_pgs,torch.device('cpu')),pgs_to_device(self.disc_master_pgs,default_device()))\n",
    "            #self.gen_master_pgs,self.disc_master_pgs=tmp\n",
    "            self.model_pgs,self.master_pgs, self.old_pgs = (self.disc_model_pgs,self.disc_master_pgs, self.disc_old_pgs)\n",
    "        else: \n",
    "            #tmp=(pgs_to_device(self.disc_master_pgs,torch.device('cpu')),pgs_to_device(self.gen_master_pgs,default_device()))\n",
    "            #self.disc_master_pgs,self.gen_master_pgs=tmp\n",
    "            self.model_pgs,self.master_pgs, self.old_pgs = (self.gen_model_pgs,self.gen_master_pgs, self.gen_old_pgs)\n",
    "        if(self.multi_loss): \n",
    "            self.loss_scale=self.dloss_scale if self.learn.disc_pass else self.gloss_scale\n",
    "            self.count=self.d_count if(self.learn.disc_pass) else self.g_count\n",
    "        \n",
    "        if(not self.learn.disc_pass and self.loss_scale<512 and (self.iter+1)%100==0):\n",
    "            self.loss_scale*=1\n",
    "        #print('self.model_pgs,self.master_pgs, self.old_pgs: '+str(self.model_pgs[0][0].device)+':'+str(self.master_pgs[0][0].device)+':'+str(self.old_pgs[0][0].device))\n",
    "        #print('Disc pass: '+str(self.learn.disc_pass)+':'+str(self.gen_master_pgs[0][0].device)+':'+str(self.disc_master_pgs[0][0].device))\n",
    "    def after_pred(self): \n",
    "        fp32=lambda t:tuple(x.float() for x in t)\n",
    "        self.learn.pred = tuple(map(fp32,self.learn.pred))\n",
    "        self.learn.yb = tuple(map(fp32,self.learn.yb))\n",
    "    def _super_after_backward(self):\n",
    "        self.learn.loss /= self.loss_scale #To record the real loss\n",
    "        #First, check for an overflow\n",
    "        dynamic_loss_overflow = (self.dynamic and (not self.multi_loss) \n",
    "                                 and grad_overflow(self.gen_model_pgs+self.disc_model_pgs)) \n",
    "        multi_loss_overflow = (self.multi_loss and (((not self.learn.disc_pass) and grad_overflow(self.gen_model_pgs)) \n",
    "                               or ( self.learn.disc_pass and grad_overflow(self.disc_model_pgs)))) #have to check both for overflow\n",
    "        if (dynamic_loss_overflow or multi_loss_overflow):\n",
    "            self.loss_scale = self.loss_scale/self.div_factor if self.loss_scale > 4 else 4\n",
    "            self.model.zero_grad()\n",
    "            self.count=0\n",
    "            self._set_gd_count()\n",
    "            raise CancelBatchException() #skip step and zero_grad\n",
    "\n",
    "        to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)\n",
    "        for master_params in self.master_pgs:\n",
    "            for param in master_params:\n",
    "                if param.grad is not None: param.grad.div_(self.loss_scale)\n",
    "        #Check if it's been long enough without overflow\n",
    "        if self.clip is not None:\n",
    "            for group in self.master_pgs: nn.utils.clip_grad_norm_(group, self.clip)\n",
    "        if self.dynamic:\n",
    "            self.count += 1\n",
    "            if self.count == self.scale_wait:\n",
    "                self.count=0\n",
    "                self.loss_scale *= self.div_factor\n",
    "            self._set_gd_count()\n",
    "            \n",
    "    def _set_gd_count(self):\n",
    "        if(self.learn.disc_pass):\n",
    "            self.d_count=self.count\n",
    "        else:\n",
    "            self.g_count=self.count\n",
    "    def after_backward(self):\n",
    "        self.learn.overflow=False\n",
    "        try:\n",
    "            self._super_after_backward()\n",
    "        except CancelBatchException as e: \n",
    "            self.learn.overflow=True\n",
    "            raise e from None\n",
    "        finally:\n",
    "            if(self.learn.disc_pass):\n",
    "                self.dloss_scale=self.loss_scale\n",
    "            else:\n",
    "                self.gloss_scale=self.loss_scale\n",
    "    def after_fit(self):\n",
    "        #need to fix this for last run\n",
    "        \n",
    "        #_copy_state(self.learn.opt, self.master_pgs, self.model_pgs)\n",
    "        #self.learn.opt.param_groups  = self.old_pgs\n",
    "        delattr(self, \"gen_master_pgs\")\n",
    "        delattr(self, \"gen_model_pgs\")\n",
    "        delattr(self, \"gen_old_pgs\")\n",
    "        \n",
    "class Perf(Callback):\n",
    "    towards_end=True\n",
    "    def after_loss(self):\n",
    "        del self.learn.xb\n",
    "        del self.learn.yb \n",
    "    \n",
    "learner.add_cbs((MixedPrecisionTuple(dynamic=True,multi_loss=True,loss_scale=1.),ModelToHalfAdaLIN()))#, max_loss_scale=((2 - 2**-23) * 2**127))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.show_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY pretrained resnet + resnet with less channels added with training only the side-by-side added resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.one_batch??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import pdb; pdb.set_trace()\n",
    "#break learner.loss_func[0].__call__\n",
    "#break learner.model.models['DA'].forward\n",
    "#import pdb; pdb.set_trace()\n",
    "learner.fit(30,wd=wd,lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda p:(p[0],p[1].flatten()[0]),learner.model.models['GA2B'].named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda p:(p[0],p[1].flatten()[0]),learner.model.models['GA2B'].named_parameters()))\n",
    "#print('planned stop')\n",
    "#a/;ihgfaih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pdb; pdb.set_trace()\n",
    "learner.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb; pdb.set_trace()\n",
    "#learner.save(path/'30_fp32_90x90_1_15')\n",
    "print('worked!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('planned stop')\n",
    "a/;ihgfaih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.cbs[-3].div_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out=(learner.model.models['GA2B'](learner.x[0])[0][0].detach().cpu()*data_stats['anime_stds'][:,None][:,None]+data_stats['anime_means'][:,None][:,None])\n",
    "out*0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out=learner.model.models['GA2B'](next(iter(learner.dbunch.valid_dl))[0][0])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(learner.model.models['LA'].named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce(lambda p1,p2:(p1 if p1[1] > p2[1] else p2),\n",
    "reduce(lambda p1,p2:(p1 if p1[2] > p2[2] else p2),list(map(lambda p:(p[0],p[1].max(),p[1].min()),learner.model.models['GB2A'].named_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm2d(3)(torch.rand([1,3,50,50]).half()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(fp16[-1].parameters()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.utils.SpectralNorm??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model[0].parameters()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torch.rand([1,3,64,64],dtype=torch.float16,device=default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nn.BatchNorm2d(10).buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm2d(3).cuda()(test).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MixedPrecision??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi(nn.Module):\n",
    "    def __init__(self,num):\n",
    "        super(multi, self).__init__()\n",
    "        self.num=nn.Parameter(torch.Tensor(num))\n",
    "    def forward(self,x):\n",
    "        return (x*self.num).to(x.dtype)\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3,10,3),\n",
    "    multi(1),\n",
    "    nn.Conv2d(10,10,3)\n",
    ")\n",
    "for layer in model:\n",
    "    if(not isinstance(layer,multi)):\n",
    "        layer.half()\n",
    "print(next(model[0].parameters()).dtype,next(model[1].parameters()).dtype)\n",
    "model=model.cuda()\n",
    "print(list(model[1].parameters()))\n",
    "opt=Adam(model.parameters(),lr=0.01)\n",
    "pred=model(test).float()\n",
    "loss=nn.MSELoss()(pred,torch.ones([1,10,60,60],device=default_device()))\n",
    "loss.backward()\n",
    "opt.step()\n",
    "print(list(model[1].parameters()),next(model[1].parameters()).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test*torch.ones([1],dtype=torch.float32).cuda()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn=nn.Sequential(\n",
    "    nn.Conv2d(3,10,3),\n",
    "    nn.ReLU(True),\n",
    "    nn.InstanceNorm2d(10,affine=True),\n",
    "    nn.Conv2d(10,100,3),\n",
    "    nn.ReLU(True),\n",
    "    nn.InstanceNorm2d(100,affine=True)\n",
    ")\n",
    "fp16=convert_network(simple_cnn, dtype=torch.float16).to(default_device())\n",
    "fp16(torch.rand([1,3,50,50]).half().to(default_device())).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_network??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to handle adalin not handling f(half)=half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break learner.model.models['GA2B'].UpBlock1_1.norm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(model[1],multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import pdb; pdb.set_trace()\n",
    "#300 total\n",
    "#import pdb; pdb.set_trace()\n",
    "learner.fit(300) #60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.one_batch??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MixedPrecision??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.disc_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder._train_mets[0].beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.save(path/'Nov_26th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = torch.hub.load('pytorch/vision:v0.4.2', 'inception_v3', pretrained=True)\n",
    "inception.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception=Inception_v3()\n",
    "state = torch.hub.load('pytorch/vision:v0.4.2', 'inception_v3', pretrained=True).state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential BUG!!!!! nn.DataStructure registers modules again, might be double updating weights!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dl is None: dl = learner.dbunch.dls[ds_idx]\n",
    "b = dl.one_batch()\n",
    "_,_,preds = learner.get_preds(dl=dl,ds_idx=1, with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=learner.dbunch.valid_dl.one_batch()\n",
    "_,_,preds = learner.get_preds(dl=[b], with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not actually useful\n",
    "learner.recorder._valid_mets[0].total=0\n",
    "learner.recorder._valid_mets[0].count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.training=False\n",
    "b = learner.dbunch.valid_dl.one_batch()\n",
    "learner.one_batch(0, b)\n",
    "preds = getattr(learner.loss_func, 'activation', noop)(learner.pred)\n",
    "out = getattr(learner.loss_func, 'decodes', noop)(preds)\n",
    "#import pdb; pdb.set_trace()\n",
    "#think i can take loss.decode -> Tensor.decode -> CycleImage.decode\n",
    "x,y,its,outs = learner.dbunch.valid_dl.show_results(b, out,show=False, max_n=9)\n",
    "#tensorboard_log(x, y, its, outs, self.writer, self.train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl=learner.dbunch.valid_dl\n",
    "its=vdl.after_batch(vdl.do_batch([vdl.do_item(0)]))\n",
    "print(mapped(type,its))\n",
    "#torch.Tensor(vdl.after_batch(vdl.do_batch([vdl.do_item(0)]))[1])\n",
    "print(CycleImage(its[0][0].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.show_batch(b_out, max_n=max_n, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[]\n",
    "for i,(xb,yb) in enumerate(iter(db.valid_dl)):\n",
    "    xbA,xbB=xb\n",
    "    fake_Bb=learner.model.models['GA2B'](xbA.half())[0].detach()\n",
    "    ins=xbA[0]\n",
    "    show_image(xbA[0])\n",
    "    samples+=[fake_Bb[0]]\n",
    "    if(i>10): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.load(Path('/home/fast/.fastai/m1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(samples)):\n",
    "    show_image(samples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from fast.notebook.export import notebook2script\n",
    "#notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIP !!! use_partial_data data source use partial data (keep pct) \n",
    "# MixedPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_open_resize(fn:Path, sz:Int=299):\n",
    "    x = Image.open(fn).resize((sz,sz))\n",
    "    # Convert image to tenshttp://127.0.0.1:8888/notebooks/anime/Dataloading.ipynb#or for modeling\n",
    "    x = array(x, dtype=np.float32)\n",
    "    #Shouldn't I be normalizing data?!??!?\n",
    "    if x.ndim==3:\n",
    "        return x/255.\n",
    "    elif x.ndim==2:\n",
    "        return x.repeat(1,1,3)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(str(path/'selfie_mmd_test_full'),array([num_open_resize(selfie_imgs[i]) for i in range(len(selfie_imgs))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(str(path/'anime_mmd_test_full'),array([num_open_resize(anime_imgs[i]) for i in range(len(anime_imgs))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#learner=Learner(db,ugatit.models,[noop_loss,noop_loss],[Adam,Adam],0.0001,splitter=multi_model_params)\n",
    "#class UGATITLearner(Learner):\n",
    "#    \"Group together a `model`, some `dbunch` and a `loss_func` to handle training\"\n",
    "#    def __init__(self, dbunch, models, loss_funcs=None, opt_funcs=[SGD,SGD], lr=None, splitter=trainable_params, cbs=None,\n",
    "#                 cb_funcs=None, metrics=None, path=None, model_dir='models', wd_bn_bias=False, train_bn=True):\n",
    "#        store_attr(self, \"dbunch,splitter,model_dir,wd_bn_bias,train_bn\")\n",
    "#        self.model_GA2B,self.model_GB2A,self.model_DA,self.model_DB,self.model_LA,self.model_LB=models\n",
    "#        self.generators=nn.ModuleList([self.model_GA2B,self.model_GB2A])\n",
    "#        self.discriminators=nn.ModuleList([self.model_DA,self.model_DB,self.model_LA,self.model_LB])\n",
    "#        self.model=nn.ModuleList([self.generators,self.discriminators])\n",
    "#        self.models=self.model\n",
    "#        self.opt_funcG,self.opt_funcD=opt_funcs\n",
    "#        self.lr = defaults.lr if lr is None else lr\n",
    "        #TODO: infer loss_func from data\n",
    "#        self.loss_funcs = L([CrossEntropyLossFlat()]) if loss_funcs is None else loss_funcs\n",
    "#        self.path = path if path is not None else getattr(dbunch, 'path', Path('.'))\n",
    "#        self.metrics = [m if isinstance(m, Metric) else AvgMetric(m) for m in L(metrics)]\n",
    "#        self.training,self.logger,self.opts = False,print,None\n",
    "#        self.cbs = L([])\n",
    "#        self.add_cbs(cbf() for cbf in L(defaults.callbacks[0])) #TODO Took out tracking due ot error\n",
    "#        self.add_cbs(cbs)\n",
    "#        self.add_cbs(cbf() for cbf in L(cb_funcs))\n",
    "#        self.ugatit_loss=UGATITLoss(models)\n",
    "#    \n",
    "#    def create_opt(self,opt_func,models):\n",
    "#        \"Create an optimizer with `lr`\"\n",
    "#        opt = opt_func(self.splitter(models), lr=self.lr)\n",
    "#        if not self.wd_bn_bias:\n",
    "#            for p in bn_bias_params(models):\n",
    "                opt.state[p] = {**opt.state.get(p, {}), 'do_wd': False}\n",
    "        if self.train_bn:\n",
    "            for p in bn_bias_params(models, with_bias=False):\n",
    "                opt.state[p] = {**opt.state.get(p, {}), 'force_train': True}\n",
    "        return opt\n",
    "    \n",
    "    def create_opts(self):\n",
    "        #generator\n",
    "        optG=self.create_opt(self.opt_funcG,self.generators)\n",
    "        #discriminator\n",
    "        optD=self.create_opt(self.opt_funcD,self.discriminators)\n",
    "        return (optG,optD)\n",
    "\n",
    "    # models? which models get which inputs?\n",
    "    def one_batch(self, i, b):\n",
    "        self.iter = i\n",
    "        try:\n",
    "            self._split(b)\n",
    "            self('begin_batch')\n",
    "            #self.pred = self.model_GA2B(self.xb[0][0]);             self('after_pred')\n",
    "            l=self.ugatit_loss.generator_loss(self.xb[0])\n",
    "            self.loss=l.item();         self('after_loss') #should use loss_func\n",
    "            if not self.training: return\n",
    "            l.backward();                           self('after_backward')\n",
    "            \n",
    "            self.opts[0].step();                                self('after_step')\n",
    "            self.opts[0].zero_grad()\n",
    "            \n",
    "            l = self.ugatit_loss.discriminator_loss(self.xb[0]); #self('after_loss2')\n",
    "            l.backward();                           #self('after_backward2')\n",
    "            \n",
    "            self.opts[1].step();                                #self('after_step2')\n",
    "            self.opts[1].zero_grad()\n",
    "            \n",
    "        except CancelBatchException:                        self('after_cancel_batch')\n",
    "        finally:                                            self('after_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
