{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.basics import *\n",
    "from fastai2.optimizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(5,100),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(100,10),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(10,100),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(100,1000)\n",
    ").cuda()\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedOptimizer(Optimizer):\n",
    "    #Changing order to guarantee order of params in optimizer smallest to largest. \n",
    "    #This allows params to be cleared before getting to largest param update. \n",
    "    def all_params(self, n=slice(None), with_grad=False, sort_key=lambda p: np.prod(p[0].shape)):\n",
    "        res = super().all_params(n=n, with_grad=with_grad)\n",
    "        res.sort(sort_key)\n",
    "        return res\n",
    "    def zero_grad(self, clear=False):\n",
    "        for p,*_ in self.all_params(with_grad=True):\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "            if(clear): del p.grad\n",
    "def zero_grad(p, **kwargs):\n",
    "    \"Register the number of steps done in `state` for `p`\"\n",
    "    p.grad.detach_()\n",
    "    p.grad.zero_()\n",
    "    del p.grad\n",
    "    return {}\n",
    "def zero_Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step,zero_grad]\n",
    "    return OptimizedOptimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "def clear_grads(params):\n",
    "    for p in params:\n",
    "        if(p.grad is not None):\n",
    "            del p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = torch.optim.Adam(model.parameters(),weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(model.parameters(),lr=0.001,wd=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2732544\n",
    "adam = zero_Adam(model.parameters(),lr=0.001,wd=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([1,5]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1670144, 2463232)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(x)\n",
    "loss = loss_func(pred,torch.ones([1],dtype=torch.long).cuda())\n",
    "loss.backward()\n",
    "adam.step()\n",
    "pred = model(x)\n",
    "loss = loss_func(pred,torch.ones([1],dtype=torch.long).cuda())\n",
    "loss.backward()\n",
    "#clear_grads(model.parameters())\n",
    "torch.cuda.memory_allocated(),torch.cuda.max_memory_allocated()\n",
    "#(1253888, 2463232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)\n",
    "loss = loss_func(pred,torch.ones([1],dtype=torch.long).cuda())\n",
    "loss.backward()\n",
    "adam.step()\n",
    "pred = model(x)\n",
    "loss = loss_func(pred,torch.ones([1],dtype=torch.long).cuda())\n",
    "loss.backward()\n",
    "adam.step()\n",
    "adam.zero_grad(clear=False)\n",
    "torch.cuda.memory_allocated(),torch.cuda.max_memory_allocated()\n",
    "#(1670144, 2471424)\n",
    "#(1253888, 2455040)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)\n",
    "loss = loss_func(pred,torch.ones([1],dtype=torch.long).cuda())\n",
    "loss.backward()\n",
    "adam.step()\n",
    "pred = model(x)\n",
    "loss = loss_func(pred,torch.ones([1],dtype=torch.long).cuda())\n",
    "loss.backward()\n",
    "adam.step()\n",
    "torch.cuda.memory_allocated(),torch.cuda.max_memory_allocated()\n",
    "#(1670144, 2471424)\n",
    "#(1253888, 2471424)\n",
    "#(1253888, 2455040)\n",
    "#(1670144, 2471424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_allocated()\n",
    "#pytorch 25123840\n",
    "#fastai 25119744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam.step()\n",
    "torch.cuda.memory_allocated(),torch.cuda.max_memory_allocated()\n",
    "#(1465344, 2662400)\n",
    "#(1706496, 2499584)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam.zero_grad()\n",
    "torch.cuda.memory_allocated(),torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8196608/25123840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_decay(p, lr, wd, do_wd=True, **kwargs):\n",
    "    \"Weight decay as decaying `p` with `lr*wd`\"\n",
    "    if do_wd and wd!=0: p.data.mul_(1 - lr*wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_grad(p, mom, dampening=False, grad_avg=None, **kwargs):\n",
    "    \"Keeps track of the avg grads of `p` in `state` with `mom`.\"\n",
    "    if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-mom if dampening else 1.\n",
    "    grad_avg.mul_(mom).add_(damp, p.grad.data)\n",
    "    return {'grad_avg': grad_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sqr_grad(p, sqr_mom, dampening=True, sqr_avg=None, **kwargs):\n",
    "    if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-sqr_mom if dampening else 1.\n",
    "    sqr_avg.mul_(sqr_mom).addcmul_(damp, p.grad.data, p.grad.data)\n",
    "    return {'sqr_avg': sqr_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_stat(p, step=0, **kwargs):\n",
    "    \"Register the number of steps done in `state` for `p`\"\n",
    "    step += 1\n",
    "    return {'step' : step}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_step(p, lr, mom, step, sqr_mom, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    \"Step for Adam with `lr` on `p`\"\n",
    "    debias1 = debias(mom,     1-mom,     step)\n",
    "    debias2 = debias(sqr_mom, 1-sqr_mom, step)\n",
    "    p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step]\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(_BaseOptimizer):\n",
    "    \"Base optimizer class for the fastai library, updating `params` with `cbs`\"\n",
    "    _keep_on_clear = ['force_train', 'do_wd']\n",
    "    def __init__(self, params, cbs, train_bn=True, **defaults):\n",
    "        params = L(params)\n",
    "        self.cbs,self.state,self.train_bn = L(cbs),defaultdict(dict),train_bn\n",
    "        defaults = merge(*self.cbs.attrgot('defaults'), defaults)\n",
    "        self.param_groups = L(L(p) for p in params) if isinstance(params[0], (L,list)) else L([params])\n",
    "        self.hypers = L({} for _ in range_of(self.param_groups))\n",
    "        self.set_hypers(**defaults)\n",
    "        self.frozen_idx = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,*_ in self.all_params(with_grad=True):\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for p,pg,state,hyper in self.all_params(with_grad=True):\n",
    "            for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper}))\n",
    "            self.state[p] = state\n",
    "\n",
    "    def clear_state(self):\n",
    "        for p,pg,state,hyper in self.all_params():\n",
    "            self.state[p] = {k: state[k] for k in self._keep_on_clear if k in state}\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = [self.state[p] for p,*_ in self.all_params()]\n",
    "        return {'state': state, 'hypers': self.hypers}\n",
    "\n",
    "    def load_state_dict(self, sd):\n",
    "        assert len(sd[\"hypers\"]) == len(self.param_groups)\n",
    "        assert len(sd[\"state\"])  == sum([len(pg) for pg in self.param_groups])\n",
    "        self.hypers = sd['hypers']\n",
    "        self.state = {p: s for p,s in zip(self.all_params().itemgot(0), sd['state'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortedOptimizer.all_params??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = iter(sorted(list(model.parameters()),\n",
    "                               key=lambda p: np.prod(p.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_Adam.all_params??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai2)",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
