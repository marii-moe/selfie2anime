{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp anime.dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.layers import *\n",
    "from fastai2.data.all import *\n",
    "from fastai2.data.block import *\n",
    "from fastai2.optimizer import *\n",
    "from fastai2.learner import *\n",
    "from fastai2.metrics import *\n",
    "from fastai2.callback.all import *\n",
    "from fastai2.vision.all import *\n",
    "from anime.ugatit import *\n",
    "from anime.ugatit_networks import ILN,adaILN\n",
    "from torch.autograd import Variable\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently not really used, will be for alternative dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path(\"/home/fast/.fastai/data/danbooru2018\")\n",
    "meta_path=path/'metadata/2018000000000016'\n",
    "portraits=pd.read_pickle(path/'portraits')\n",
    "selfie_path=Path('/home/fast/.fastai/data/Selfie-dataset')\n",
    "self_txt= selfie_path/'selfie_dataset.txt'\n",
    "attrs='image_name popularity partial_faces is_female baby child teenager youth middle_age senior white black asian oval_face round_face heart_face smiling mouth_open frowning wearing_glasses wearing_sunglasses wearing_lipstick tongue_out duck_face black_hair blond_hair brown_hair red_hair curly_hair straight_hair braid_hair showing_cellphone using_earphone using_mirror braces wearing_hat harsh_lighting dim_lighting'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _default_ugatit_datastats():\n",
    "        data_stats={'selfie_means':torch.Tensor([143.81911227601947, 119.46716940677527, 113.92494676532168]),\n",
    "         'selfie_stds':torch.Tensor([67.72407215465199, 64.41496156519109, 52.516515323610015]),\n",
    "         'anime_means':torch.Tensor([173.24273681640625, 155.69161987304688, 135.62557983398438]),\n",
    "         'anime_stds':torch.Tensor([51.790767669677734, 48.620208740234375, 48.380496978759766])}\n",
    "        return {'means':(data_stats['selfie_means']+data_stats['anime_means'])/2,\n",
    "             'stds':(data_stats['selfie_stds']+data_stats['anime_stds'])/2,\n",
    "             'real_stats':data_stats}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are currently not used, but can be used to access dataframes that other _danbooru dataset was generated from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSelfiePath(df,dir,i):\n",
    "    dft=df.iloc[[i]]\n",
    "    path=dir/((dft['image_name']).values[0]+'.jpg')\n",
    "    return (dft.values,path)\n",
    "def getImagePath(df,dir,i):\n",
    "    dft=df.iloc[[i]]\n",
    "    path=dir/'512px'/(dft['directory']+'/'+dft['file']).values[0]\n",
    "    return (dft.tags.values[0],path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DSList():\n",
    "    def __init__(self,name='selfie2anime'):\n",
    "        self.name=name\n",
    "    def __call__(self):\n",
    "        if(self.name=='selfie2anime'):\n",
    "            return self._selfie2anime()\n",
    "        elif(self.name=='_danbooru'):\n",
    "            return self._danbooru()\n",
    "        elif(self.name=='selfie2animeTest'):\n",
    "            return self._selfie2animeTest()\n",
    "    def _danbooru(self):\n",
    "        def txt_to_path(img,dir=dir,ext='.jpg'):\n",
    "            return dir/(img+ext)\n",
    "        selfies=pd.read_pickle(selfie_path/'selfies')\n",
    "        selfie_imgs=L(list(selfies['image_name'].apply(partial(txt_to_path,dir=selfie_path/'images'))))\n",
    "        f=partial(txt_to_path,dir=path/'512px',ext='')\n",
    "        locs=portraits['directory']+'/'+portraits['file']\n",
    "        anime_imgs=L(list(locs.apply(f)))\n",
    "        return selfie_imgs,anime_imgs\n",
    "    def _selfie2anime(self):\n",
    "        selfie_imgs=L(Path(\"/home/fast/.fastai/data/selfie2anime/trainA\").ls())\n",
    "        anime_imgs=L(Path(\"/home/fast/.fastai/data/selfie2anime/trainB\").ls())\n",
    "        return selfie_imgs,anime_imgs\n",
    "    def _selfie2animeTest(self):\n",
    "        selfie_imgs=L(Path(\"/home/fast/.fastai/data/selfie2anime/testA\").ls())\n",
    "        anime_imgs=L(Path(\"/home/fast/.fastai/data/selfie2anime/testB\").ls())\n",
    "        return selfie_imgs,anime_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfies,animes=DSList()()\n",
    "assert(len(animes)==3400)\n",
    "assert(len(selfies)==3400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "denorm = lambda x,data_stats: (x*data_stats['stds'][...,None,None]+data_stats['means'][...,None,None])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CycleImage(Tuple):\n",
    "    def toTensor(self):\n",
    "        selfie,anime = self\n",
    "        return torch.cat([selfie,anime], dim=2)\n",
    "    def show(self, ctx=None,data_stats=_default_ugatit_datastats(), **kwargs): \n",
    "        img=self.toTensor().detach().cpu()\n",
    "        half=int(img.shape[2]/2)\n",
    "        selfie=denorm(img[:,:,0:half],data_stats)\n",
    "        anime=denorm(img[:,:,half:],data_stats)\n",
    "        return show_image(torch.cat([selfie,anime], dim=2),  ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CyclePair(Transform):\n",
    "    def __init__(self,bItems):\n",
    "        self.small = bItems \n",
    "        self.assoc = self\n",
    "        \n",
    "    def encodes(self,i):\n",
    "        \"x: tuple of `i`th image and a random image from same or different class; y: True if same class\"\n",
    "        return CycleImage(i, random.choice(self.small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToTensorTuple(Transform):\n",
    "    \"Convert item to appropriate tensor class\"\n",
    "    order = 15\n",
    "    def encodes(self, o:CycleImage): return TensorImage(image2byte(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resized_image(fn:Path, sz=128):\n",
    "    x = Image.open(fn).resize((sz,sz))\n",
    "    x = array(x)\n",
    "    if x.ndim==3:\n",
    "        return tensor(x).permute(2,0,1).float()/255.\n",
    "    elif x.ndim==2:\n",
    "        return tensor(x)[None].repeat(3,1,1).float()/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ListofTransformsforTuple(Transform):\n",
    "    \"Convert item to appropriate tensor class\"\n",
    "    order = 60\n",
    "    def __init__(self, transforms, stats,norm):\n",
    "        self.transforms=transforms\n",
    "        self.stats=to_device(stats)\n",
    "        self.norm=norm\n",
    "    def encodes(self, o): \n",
    "        if(o.shape[-3:]==torch.Size([1, 1, 1])): #don't transform the labels\n",
    "            return o\n",
    "        o=TensorImage(o)\n",
    "        with torch.no_grad():\n",
    "            for t in self.transforms:\n",
    "                o=t(o)\n",
    "            o = self.norm(to_device(self.stats,device=o.device),o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NoopTensor(Transform):\n",
    "    def encodes(self, x): return CycleImage(torch.Tensor([[[0]]]),torch.Tensor([[[0]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:CycleImage, y:CycleImage, samples, ctxs=None, max_n=9, **kwargs):\n",
    "    if ctxs is None: ctxs = Inf.nones\n",
    "    for i in range_of(samples[0]):\n",
    "        [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))]\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UGATITData():\n",
    "    def __init__(self,selfie_list=DSList()()[0],anime_list=DSList()()[1],img_size=256):\n",
    "        self.selfie_list,self.anime_list=selfie_list,anime_list\n",
    "        self.data_stats=_default_ugatit_datastats()\n",
    "        self.img_size=img_size\n",
    "    def create_ugatit_dls(self,sample_size=3400,val=None,bs=3,augmentations=aug_transforms()):\n",
    "        val=ifnone(val,['female_10686.jpg','female_10367.jpg','female_30988.jpg','female_8194.jpg','female_18724.jpg','female_10087.jpg','female_27647.jpg','female_13409.jpg','female_20608.jpg','female_26279.jpg','female_7103.jpg','female_10615.jpg','female_25884.jpg','female_31235.jpg','female_28991.jpg','female_9936.jpg','female_33285.jpg','female_33491.jpg','female_1597.jpg', 'female_13893.jpg', 'female_26481.jpg', 'female_32384.jpg', 'female_18827.jpg', 'female_19176.jpg', 'female_6150.jpg','female_1487.jpg', 'female_13914.jpg', 'female_29264.jpg', 'female_17571.jpg', 'female_32817.jpg', 'female_7374.jpg', 'female_16798.jpg', 'female_21739.jpg', 'female_22605.jpg', 'female_21305.jpg', 'female_12988.jpg', 'female_4297.jpg', 'female_9025.jpg', 'female_13560.jpg', 'female_31712.jpg', 'female_1177.jpg', 'female_13787.jpg', 'female_11117.jpg', 'female_10664.jpg', 'female_17134.jpg', 'female_16420.jpg', 'female_95.jpg', 'female_15298.jpg', 'female_32776.jpg', 'female_9353.jpg', 'female_22181.jpg', 'female_32015.jpg', 'female_27565.jpg', 'female_19001.jpg', 'female_8955.jpg', 'female_33094.jpg', 'female_7486.jpg', 'female_32167.jpg', 'female_28716.jpg', 'female_6519.jpg', 'female_4339.jpg', 'female_24557.jpg', 'female_20786.jpg', 'female_26695.jpg', 'female_25546.jpg', 'female_15424.jpg', 'female_711.jpg', 'female_31004.jpg', 'female_9440.jpg', 'female_22912.jpg', 'female_2754.jpg', 'female_24839.jpg', 'female_12741.jpg', 'female_9480.jpg', 'female_15677.jpg', 'female_23557.jpg', 'female_3006.jpg', 'female_29858.jpg', 'female_8664.jpg', 'female_16808.jpg', 'female_32694.jpg', 'female_5019.jpg', 'female_15584.jpg', 'female_32256.jpg', 'female_5747.jpg', 'female_22101.jpg', 'female_4115.jpg', 'female_20185.jpg', 'female_25131.jpg', 'female_31061.jpg', 'female_2546.jpg', 'female_30868.jpg', 'female_17341.jpg', 'female_9415.jpg', 'female_14142.jpg', 'female_11470.jpg', 'female_683.jpg', 'female_3433.jpg', 'female_19286.jpg', 'female_30070.jpg'])\n",
    "        tfms = Transform(partial(resized_image,sz=self.img_size))\n",
    "        tfms = Pipeline([CyclePair(self.anime_list), tfms, IntToFloatTensor(div=False)])\n",
    "        noop = Pipeline([NoopTensor()])\n",
    "        val_path=self.selfie_list[0].parent\n",
    "        sample_size=len(self.selfie_list)-100 if sample_size<3 or sample_size>len(self.selfie_list)-100 else sample_size\n",
    "        selfie_sample=self.selfie_list.filter(lambda s:s.name in val,negate=True)\n",
    "        selfie_sample=selfie_sample.shuffle()[0:sample_size]\n",
    "        selfie_sample=selfie_sample+map(lambda v:val_path/v,val)\n",
    "        tfmDs=Datasets(selfie_sample,[tfms,noop],splits=[[selfie.name not in val for selfie in selfie_sample],[selfie.name in val for selfie in selfie_sample]])\n",
    "        augmentation=augmentations\n",
    "        augmentation=ListofTransformsforTuple(augmentation, self.data_stats, self.norm)\n",
    "        return tfmDs.dataloaders(bs=bs,val_bs=1,after_batch=[augmentation], num_workers=0)\n",
    "    def norm(self,stats,x): return (255*x-unsqueeze(stats['means'],n=2)[None])/unsqueeze(stats['stds'],n=2)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=UGATITData().create_ugatit_dls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai2)",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
