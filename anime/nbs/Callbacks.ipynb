{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp anime.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.layers import *\n",
    "from fastai2.data.all import *\n",
    "from fastai2.data.block import *\n",
    "from fastai2.optimizer import *\n",
    "from fastai2.learner import *\n",
    "from fastai2.metrics import *\n",
    "from fastai2.callback.all import *\n",
    "from fastai2.vision.all import *\n",
    "from anime.ugatit import *\n",
    "from fastai2.callback.wandb import WandbCallback\n",
    "import wandb\n",
    "from fastai2.fp16_utils import convert_module\n",
    "from anime.ugatit_networks import ILN,adaILN\n",
    "from fastai2.callback.fp16 import _copy_state\n",
    "from torch.autograd import Variable\n",
    "from anime.dataloading import CycleImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GANOptimizer(object):\n",
    "    def __init__(self, gen_optim,disc_optim,params,**kwargs):\n",
    "        gen_params,disc_params=params\n",
    "        #might want to try lower lr for the generator\n",
    "        self.gen_optim,self.disc_optim=gen_optim(gen_params,**kwargs),disc_optim(disc_params,**kwargs)\n",
    "        self.optimizing_gen=True\n",
    "    def __getattr__(self, item):\n",
    "        opt= self.gen_optim if(self.optimizing_gen) else self.disc_optim\n",
    "        return getattr(opt, item)\n",
    "    def zero_grad(self):\n",
    "        self.gen_optim.zero_grad()\n",
    "        self.disc_optim.zero_grad()\n",
    "    def state_dict(self):\n",
    "        gs = self.gen_optim.state_dict()\n",
    "        ds = self.disc_optim.state_dict()\n",
    "        return {'hypers':{\n",
    "                    'gen':gs['hypers'],\n",
    "                    'disc':ds['hypers']},\n",
    "                'state':{\n",
    "                    'gen':gs['state'],\n",
    "                    'disc':ds['state']}}\n",
    "    def load_state_dict(self, sdict):\n",
    "        self.gen_optim.load_state_dict({'hypers': sdict['param_groups']['gen'],'state':sdict['state']['gen']})\n",
    "        self.disc_optim.load_state_dict({'param_groups': sdict['param_groups']['disc'],'state':sdict['state']['disc']})\n",
    "#export\n",
    "class Switch(Callback):\n",
    "    toward_end=True\n",
    "    def begin_epoch(self):\n",
    "        self.learn.disc_pass=False #TODO too many (not disc_pass), switch to gen_pass?!?\n",
    "        self.model.optimizing_gen= not self.learn.disc_pass\n",
    "        if(hasattr(self, 'gan_losses')):\n",
    "            self.learn.loss_func=self.gan_losses[self.learn.disc_pass]\n",
    "        self.opt.zero_grad()\n",
    "    def begin_batch(self):\n",
    "        if(not hasattr(self, 'gan_losses')):\n",
    "            self.gan_losses=self.loss_func\n",
    "            self.learn.loss_func=self.gan_losses[self.learn.disc_pass]\n",
    "        self.opt.zero_grad()\n",
    "        clear_grads(self.model.parameters())\n",
    "        self.learn.loss_func=self.gan_losses[self.learn.disc_pass]\n",
    "    def after_batch(self):\n",
    "        if(getattr(self.learn,'overflow',False)):\n",
    "            self.learn.overflow=False\n",
    "            if(self.learn.disc_pass):\n",
    "                self.one_batch(self.iter,self.learn.dls.train.one_batch())\n",
    "            return\n",
    "        self._switch()\n",
    "        \n",
    "    def after_epoch(self):\n",
    "        self.loss_func=self.gan_losses\n",
    "    \n",
    "    def _switch(self):\n",
    "        if(self.learn.training): self.learn.disc_pass=not self.learn.disc_pass\n",
    "        self.model.optimizing_gen= not self.learn.disc_pass\n",
    "        self.opt.optimizing_gen= not self.learn.disc_pass\n",
    "        self.opt.zero_grad()\n",
    "        self.learn.loss_func=self.gan_losses[self.learn.disc_pass]\n",
    "        if(self.learn.disc_pass):\n",
    "            self.one_batch(self.iter,(self.learn.xb[0],(CycleImage(torch.Tensor([[[[0]]]]).cuda(),torch.Tensor([[[[0]]]]).cuda()))))\n",
    "#export\n",
    "class LinearDecreaseLR(Callback):\n",
    "    def __init__(self,start_epoch=0,end_epoch=None):\n",
    "        store_attr(self,\"start_epoch,end_epoch\")\n",
    "    def begin_fit(self):\n",
    "        self.LDmax_lr=self.lr\n",
    "    def end_fit(self):\n",
    "        self.lr=self.LDmax_lr\n",
    "    def begin_batch(self):\n",
    "        end_epoch = self.end_epoch if(self.end_epoch) else self.n_epoch\n",
    "        current_iter=(self.epoch+self.start_epoch)*self.n_iter+self.iter\n",
    "        run_length=end_epoch*self.n_iter\n",
    "        half_run=run_length // 2\n",
    "        self.LDlr=self.LDmax_lr\n",
    "        if current_iter > half_run:\n",
    "            self.LDlr = (run_length-current_iter)*self.LDmax_lr/(run_length-half_run)\n",
    "        #elif current_iter < half_run/2:\n",
    "        #    self.lr = self.max_lr*(current_iter+1)/(half_run/2+1)\n",
    "        else:\n",
    "            self.LDlr = self.LDmax_lr\n",
    "        self.opt.set_hyper('lr',self.LDlr)\n",
    "#export        \n",
    "class SameXbYb(Callback):\n",
    "    def begin_batch(self):\n",
    "        self.learn.xb=(self.learn.xb[0],to_device(self.learn.xb,torch.device('cuda',index=1))[0])\n",
    "    def after_pred(self):\n",
    "        self.learn.yb=to_float(self.learn.xb)\n",
    "#export\n",
    "class CachedMemoryDebug(Callback):\n",
    "    def after_batch(self):\n",
    "        print(torch.cuda.memory_cached(device=torch.device('cuda',index=0))\n",
    "              ,torch.cuda.memory_cached(device=torch.device('cuda',index=1)))\n",
    "#export\n",
    "class OptimizedOptimizer(Optimizer):\n",
    "    #Changing order to guarantee order of params in optimizer smallest to largest. \n",
    "    #This allows params to be cleared before getting to largest param update. \n",
    "    def all_params(self, n=slice(None), with_grad=False, sort_key=lambda p: np.prod(p[0].shape)):\n",
    "        res = super().all_params(n=n, with_grad=with_grad)\n",
    "        res.sort(sort_key)\n",
    "        return res\n",
    "    def zero_grad(self, clear=False):\n",
    "        for p,*_ in self.all_params(with_grad=True):\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "            if(clear): del p.grad\n",
    "#export\n",
    "def zero_grad(p, **kwargs):\n",
    "    \"Register the number of steps done in `state` for `p`\"\n",
    "    p.grad.detach_()\n",
    "    p.grad.zero_()\n",
    "    del p.grad\n",
    "    return {}\n",
    "#export\n",
    "def zero_Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-5, wd=0., decouple_wd=True):\n",
    "    \"A `Optimizer` for Adam with `lr`, `mom`, `sqr_mom`, `eps` and `params`\"\n",
    "    cbs = [weight_decay]\n",
    "    cbs += [partial(average_grad, dampening=True), average_sqr_grad, step_stat, adam_step,zero_grad]\n",
    "    return OptimizedOptimizer(params, cbs, lr=lr, mom=mom, sqr_mom=sqr_mom, eps=eps, wd=wd)\n",
    "#export\n",
    "def clear_grads(params):\n",
    "    for p in params:\n",
    "        if(is_listy(p)):\n",
    "            clear_grads(p)\n",
    "        elif(p.grad is not None):\n",
    "            del p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def wandb_process(x:CycleImage, y:CycleImage, samples, outs):\n",
    "    \"Process `sample` and `out` depending on the type of `x/y`\"\n",
    "    res = []\n",
    "    for s,o in zip(samples, outs):\n",
    "        img = TensorImage(s[0].toTensor()[0])\n",
    "        #db.valid_dl.decode(s)\n",
    "        res.append(wandb.Image(img, caption='Input data', grouping=3))\n",
    "        for t, capt in ((o[0], \"Prediction\"), (s[1], \"Ground Truth\")):\n",
    "            # Resize plot to image resolution (from https://stackoverflow.com/a/13714915)\n",
    "            my_dpi = 100\n",
    "            fig = plt.figure(frameon=False, dpi=my_dpi)\n",
    "            h, w = img.shape[:2]\n",
    "            fig.set_size_inches(w / my_dpi, h / my_dpi)\n",
    "            ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            # Superimpose label or prediction to input image\n",
    "            ax = img.show(ctx=ax)\n",
    "            #import pdb; pdb.set_trace()\n",
    "            ax = t.show(ctx=ax)\n",
    "            res.append(wandb.Image(fig, caption=capt))\n",
    "            plt.close(fig)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MetricRecorder(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.remove_on_fetch=True\n",
    "    def after_batch(self):\n",
    "        mets = self.recorder._train_mets if self.training else self.recorder._valid_mets\n",
    "        self.learn.metric_values=dict([(met.name, met.value) for met in mets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Conditional_Loss(AvgSmoothLoss):\n",
    "    def __init__(self, f=lambda x:True,name='None',beta=0.98,): \n",
    "        super(Conditional_Loss,self).__init__(beta)\n",
    "        self.f=f\n",
    "        self.metric_name=name\n",
    "    def accumulate(self, learn):\n",
    "        if(self.f(learn)):\n",
    "            super().accumulate(learn)\n",
    "            \n",
    "            setattr(learn, self.metric_name, self.value)\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.metric_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/NVIDIA/apex/blob/03a25ba8a61d15a503405ef4bbda83724bb531b2/apex/fp16_utils/fp16util.py\n",
    "# had to edit due to custom normalization layers\n",
    "#export\n",
    "def convert_module_alt(module, dtype):\n",
    "    \"\"\"\n",
    "    Converts a module's immediate parameters and buffers to dtype.\n",
    "    \"\"\"\n",
    "    for param in module.parameters(recurse=False):\n",
    "        if param is not None:\n",
    "            if param.data.dtype.is_floating_point:\n",
    "                param.data = param.data.to(dtype=dtype)\n",
    "            if param._grad is not None and param._grad.data.dtype.is_floating_point:\n",
    "                param._grad.data = param._grad.data.to(dtype=dtype)\n",
    "    spectral_norm_buffers=filter(lambda b:b[0] in ('weight_u','weight_v','weight_orig'),module.named_buffers(recurse=False))\n",
    "    spectral_norm_buffers=list(map(lambda b: b[1],spectral_norm_buffers))\n",
    "    spectral_norm_buffers=[]\n",
    "    for buf in module.buffers(recurse=False):\n",
    "        if buf is not None and buf.data.dtype.is_floating_point and 0==len(list(filter(lambda b:b.shape==buf.shape and torch.all(torch.eq(b, buf)),spectral_norm_buffers))):\n",
    "            buf.data = buf.data.to(dtype=dtype)\n",
    "#export\n",
    "def convert_network(network, dtype):\n",
    "    \"\"\"\n",
    "    Converts a network's parameters and buffers to dtype.\n",
    "    \"\"\"\n",
    "    for module in network.modules():\n",
    "        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm) and module.affine is True:\n",
    "            continue\n",
    "        if isinstance(module, (ILN,adaILN,nn.InstanceNorm2d,nn.LayerNorm)):\n",
    "            continue\n",
    "        convert_module_alt(module, dtype)\n",
    "        if isinstance(module, torch.nn.RNNBase) or isinstance(module, torch.nn.modules.rnn.RNNBase):\n",
    "            module.flatten_parameters()\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def GAN_splitter(model):\n",
    "    gen_params=itertools.chain(model.models['GA2B'].parameters(),model.models['GB2A'].parameters())\n",
    "    disc_params=itertools.chain(model.models['DA'].parameters(),model.models['DB'].parameters(),model.models['LA'].parameters(),model.models['LB'].parameters())\n",
    "    return (gen_params,disc_params)\n",
    "#export\n",
    "class GANWandb(WandbCallback):\n",
    "    run_before=Switch\n",
    "    def after_batch(self):\n",
    "        \"Log hyper-parameters and training loss\"\n",
    "        g_scale,d_scale = (self.learn.mixed_precision_tuple.gloss_scale,self.learn.mixed_precision_tuple.dloss_scale) if(hasattr(self.learn,'mixed_precision_tuple')) else (1,1)\n",
    "        if self.training and not self.learn.disc_pass:\n",
    "            self._wandb_step += 1\n",
    "            self._wandb_epoch += 1/self.n_iter\n",
    "            hypers = {f'{k}G_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "            wandb.log({'epoch': self._wandb_epoch,'loss': self.smooth_loss, **hypers, **self.learn.metric_values, 'loss_scaleG': g_scale}, step=self._wandb_step)\n",
    "        elif self.training and self.learn.disc_pass:\n",
    "            hypers = {f'{k}D_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "            wandb.log({'epoch': self._wandb_epoch,'loss': self.smooth_loss, **hypers, **self.learn.metric_values, 'loss_scaleD': d_scale}, step=self._wandb_step)\n",
    "    #Fixing memory usage\n",
    "    def after_epoch(self):\n",
    "        \"Log validation loss and custom metrics & log prediction samples\"\n",
    "        # Correct any epoch rounding error and overwrite value\n",
    "        self._wandb_epoch = round(self._wandb_epoch)\n",
    "        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n",
    "        # Log sample predictions\n",
    "        if self.log_preds:\n",
    "            b = self.valid_dl.one_batch()\n",
    "            with torch.no_grad():\n",
    "                self.learn.model.eval()\n",
    "                self.learn.one_batch(0, b)\n",
    "                self.learn.model.train()\n",
    "            preds = getattr(self.loss_func, 'activation', noop)(self.pred)\n",
    "            out = getattr(self.loss_func, 'decodes', noop)(preds)\n",
    "            x,y,its,outs = self.valid_dl.show_results(b, out, show=False, max_n=self.n_preds)\n",
    "            wandb.log({\"Prediction Samples\": wandb_process(x, y, its, outs)}, step=self._wandb_step)\n",
    "        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}, step=self._wandb_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using just the generator, discriminator, and pre-trained GAN output. Loss function would be adverserial loss + MSE-ish loss with pre-trained GANs output. LR would have to decrease overtime and/or wight between adv loss and MSE loss would need to be more adverserial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_master(opt, flat_master=False, ): #device=cuda\n",
    "    model_params = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]\n",
    "    if flat_master:\n",
    "        master_params = []\n",
    "        for pg in model_params:\n",
    "            mp = parameters_to_vector([param.data.float() for param in pg])\n",
    "            mp = nn.Parameter(mp, requires_grad=True)\n",
    "            if mp.grad is None: mp.grad = mp.new(*mp.size())\n",
    "            master_params.append([mp])\n",
    "    else:\n",
    "        master_params = [[nn.Parameter(param.data.float().detach(), requires_grad=True) for param in pg] for pg in model_params]\n",
    "    return model_params, master_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clear_to_master_grads(model_pgs, master_pgs, flat_master=False):\n",
    "    for (model_params,master_params) in zip(model_pgs,master_pgs):\n",
    "        for model, master in zip(model_params, master_params):\n",
    "            if model.grad is not None:\n",
    "                if master.grad is None:\n",
    "                    master.grad = Variable(master.data.new(*master.data.size()))\n",
    "                master.grad.data.copy_(model.grad.data)\n",
    "                del model.grad\n",
    "            else:\n",
    "                master.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ModelToHalfAdaLIN(Callback):\n",
    "    \"Use with MixedPrecision callback (but it needs to run at the very beginning)\"\n",
    "    run_before=TrainEvalCallback\n",
    "    def begin_batch(self): self.learn.xb = to_half(self.learn.xb)\n",
    "#export\n",
    "class MixedPrecisionTuple(MixedPrecision):\n",
    "    def __init__(self,multi_loss=False,gdiv_factor=2,ddiv_factor=2,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multi_loss=multi_loss\n",
    "        self.gloss_scale=self.loss_scale\n",
    "        self.dloss_scale=self.loss_scale\n",
    "        self.gdiv_factor=gdiv_factor\n",
    "        self.ddiv_factor=ddiv_factor\n",
    "    def begin_fit(self):\n",
    "        if self.learn.opt is None: self.learn.create_opt()\n",
    "        self.gen_model_pgs,self.gen_master_pgs = get_master(self.opt.gen_optim, self.flat_master)\n",
    "        #Changes the optimizer so that the optimization step is done in FP32.\n",
    "        _copy_state(self.learn.opt.gen_optim, self.gen_model_pgs, self.gen_master_pgs)\n",
    "        self.disc_model_pgs,self.disc_master_pgs = get_master(self.opt.disc_optim, self.flat_master)\n",
    "        #Changes the optimizer so that the optimization step is done in FP32.\n",
    "        _copy_state(self.learn.opt.disc_optim, self.disc_model_pgs, self.disc_master_pgs)\n",
    "\n",
    "        if self.dynamic: \n",
    "            self.g_count = self.d_count = self.count = 0\n",
    "    def begin_batch(self):\n",
    "        if(self.learn.disc_pass):\n",
    "            self.model_pgs,self.master_pgs= (self.disc_model_pgs,self.disc_master_pgs)\n",
    "        else: \n",
    "            self.model_pgs,self.master_pgs = (self.gen_model_pgs,self.gen_master_pgs)\n",
    "        if(self.multi_loss): \n",
    "            self.loss_scale=self.dloss_scale if self.learn.disc_pass else self.gloss_scale\n",
    "            self.div_factor=self.ddiv_factor if self.learn.disc_pass else self.gdiv_factor\n",
    "            self.count=self.d_count if(self.learn.disc_pass) else self.g_count\n",
    "    def after_pred(self): \n",
    "        self.learn.pred = to_float(self.learn.pred)\n",
    "    def find_overflow(self,ps):\n",
    "        overflow=[]\n",
    "        for n,p in ps:\n",
    "            if p.grad is not None and test_overflow(p.grad.data): overflow.append((n,p.detach().size()))\n",
    "        return overflow\n",
    "    def _super_after_backward(self):\n",
    "        self.learn.loss /= self.loss_scale #To record the real loss\n",
    "        #First, check for an overflow\n",
    "        dynamic_loss_overflow = (self.dynamic and (not self.multi_loss) \n",
    "                                 and grad_overflow(self.gen_model_pgs+self.disc_model_pgs)) \n",
    "        multi_loss_overflow = (self.multi_loss and (((not self.learn.disc_pass) and grad_overflow(self.gen_model_pgs)) \n",
    "                               or ( self.learn.disc_pass and grad_overflow(self.disc_model_pgs)))) #have to check both for overflow\n",
    "        if (dynamic_loss_overflow or multi_loss_overflow):\n",
    "            self.loss_scale = self.loss_scale/self.div_factor if self.loss_scale/self.div_factor >= 4 else 4\n",
    "            self.count=0\n",
    "            print(self.find_overflow(self.model.named_parameters()))\n",
    "            self.model.zero_grad()\n",
    "            self._set_gd_count()\n",
    "            raise CancelBatchException() #skip step and zero_grad\n",
    "        if(self.learn.disc_pass):\n",
    "            clear_grads(self.gen_master_pgs)\n",
    "            clear_grads([self.learn.model.models['GA2B'].parameters(),self.learn.model.models['GB2A'].parameters()])\n",
    "        else:\n",
    "            clear_grads(self.disc_master_pgs)\n",
    "            clear_grads([self.learn.model.models['DA'].parameters(),self.learn.model.models['DB'].parameters(),\n",
    "                         self.learn.model.models['LA'].parameters(),self.learn.model.models['LB'].parameters()])\n",
    "        \n",
    "        clear_to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)\n",
    "        for master_params in self.master_pgs:\n",
    "            for param in master_params:\n",
    "                if param.grad is not None: param.grad.div_(self.loss_scale)\n",
    "        #Check if it's been long enough without overflow\n",
    "        if self.clip is not None:\n",
    "            for group in self.master_pgs: nn.utils.clip_grad_norm_(group, self.clip)\n",
    "        if self.dynamic:\n",
    "            self.count += 1\n",
    "            if self.count == self.scale_wait:\n",
    "                self.count=0\n",
    "                self.loss_scale *=self.div_factor\n",
    "            self._set_gd_count()\n",
    "            \n",
    "    def _set_gd_count(self):\n",
    "        if(self.learn.disc_pass):\n",
    "            self.d_count=self.count\n",
    "        else:\n",
    "            self.g_count=self.count\n",
    "    def after_backward(self):\n",
    "        self.learn.overflow=False\n",
    "        try:\n",
    "            for y in self.learn.yb:\n",
    "                del y\n",
    "            self._super_after_backward()\n",
    "            self.model.zero_grad()\n",
    "            clear_grads(self.model.parameters())\n",
    "        except CancelBatchException as e: \n",
    "            self.learn.overflow=True\n",
    "            self.model.zero_grad()\n",
    "            clear_grads(self.model.parameters())\n",
    "            raise e from None\n",
    "        finally:\n",
    "            if(self.learn.disc_pass):\n",
    "                self.dloss_scale=self.loss_scale\n",
    "            else:\n",
    "                self.gloss_scale=self.loss_scale\n",
    "    def after_fit(self):\n",
    "        _copy_state(self.learn.opt.disc_optim, self.disc_master_pgs, self.disc_model_pgs)\n",
    "        delattr(self, \"disc_master_pgs\")\n",
    "        delattr(self, \"disc_model_pgs\")\n",
    "        _copy_state(self.learn.opt.gen_optim, self.gen_master_pgs, self.gen_model_pgs)\n",
    "        delattr(self, \"gen_master_pgs\")\n",
    "        delattr(self, \"gen_model_pgs\")\n",
    "        delattr(self, \"master_pgs\")\n",
    "        delattr(self, \"model_pgs\")\n",
    "#export        \n",
    "class Perf(Callback):\n",
    "    towards_end=True\n",
    "    def after_loss(self):\n",
    "        del self.learn.xb\n",
    "        del self.learn.yb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_overflow??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MixedPrecisionTupleNo32(MixedPrecision):\n",
    "    name='mixed_precision_tuple'\n",
    "    def __init__(self,multi_loss=False,gdiv_factor=2,ddiv_factor=2,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multi_loss=multi_loss\n",
    "        self.gloss_scale=self.loss_scale\n",
    "        self.dloss_scale=self.loss_scale\n",
    "        self.gdiv_factor=gdiv_factor\n",
    "        self.ddiv_factor=ddiv_factor\n",
    "    def begin_fit(self):\n",
    "        if self.learn.opt is None: self.learn.create_opt()\n",
    "        self.gen_model_pgs = self.opt.gen_optim.param_groups\n",
    "        self.disc_model_pgs = self.opt.disc_optim.param_groups\n",
    "\n",
    "        if self.dynamic: \n",
    "            self.g_count = self.d_count = self.count = 0\n",
    "    def begin_batch(self):\n",
    "        if(self.learn.disc_pass):\n",
    "            self.model_pgs= self.disc_model_pgs\n",
    "        else: \n",
    "            self.model_pgs = self.gen_model_pgs\n",
    "        if(self.multi_loss): \n",
    "            self.loss_scale=self.dloss_scale if self.learn.disc_pass else self.gloss_scale\n",
    "            self.div_factor=self.ddiv_factor if self.learn.disc_pass else self.gdiv_factor\n",
    "            self.count=self.d_count if(self.learn.disc_pass) else self.g_count\n",
    "    def after_pred(self): \n",
    "        self.learn.pred = to_float(self.learn.pred)\n",
    "    def find_overflow(self,ps):\n",
    "        overflow=[]\n",
    "        for n,p in ps:\n",
    "            if p.grad is not None and test_overflow(p.grad.data): overflow.append((n,p.detach().size()))\n",
    "        return overflow\n",
    "    def _super_after_backward(self):\n",
    "        self.learn.loss /= self.loss_scale #To record the real loss\n",
    "        #First, check for an overflow\n",
    "        dynamic_loss_overflow = (self.dynamic and (not self.multi_loss) \n",
    "                                 and grad_overflow(self.gen_model_pgs+self.disc_model_pgs)) \n",
    "        multi_loss_overflow = (self.multi_loss and (((not self.learn.disc_pass) and grad_overflow(self.gen_model_pgs)) \n",
    "                               or ( self.learn.disc_pass and grad_overflow(self.disc_model_pgs)))) #have to check both for overflow\n",
    "        if (dynamic_loss_overflow or multi_loss_overflow):\n",
    "            self.loss_scale = self.loss_scale/self.div_factor if self.loss_scale/self.div_factor >= 4 else 4\n",
    "            self.count=0\n",
    "            print(self.find_overflow(self.model.named_parameters()))\n",
    "            self.model.zero_grad()\n",
    "            self._set_gd_count()\n",
    "            raise CancelBatchException() #skip step and zero_grad\n",
    "        \n",
    "        for params in self.model_pgs:\n",
    "            for param in params:\n",
    "                if param.grad is not None: param.grad.div_(self.loss_scale)\n",
    "        #Check if it's been long enough without overflow\n",
    "        if self.clip is not None:\n",
    "            for group in self.model_pgs: nn.utils.clip_grad_norm_(group, self.clip)\n",
    "        if self.dynamic:\n",
    "            self.count += 1\n",
    "            if self.count == self.scale_wait:\n",
    "                self.count=0\n",
    "                self.loss_scale *=self.div_factor\n",
    "            self._set_gd_count()\n",
    "            \n",
    "    def _set_gd_count(self):\n",
    "        if(self.learn.disc_pass):\n",
    "            self.d_count=self.count\n",
    "        else:\n",
    "            self.g_count=self.count\n",
    "    def after_backward(self):\n",
    "        self.learn.overflow=False\n",
    "        try:\n",
    "            for y in self.learn.yb:\n",
    "                del y\n",
    "            self._super_after_backward()\n",
    "            #self.model.zero_grad()\n",
    "        except CancelBatchException as e: \n",
    "            self.learn.overflow=True\n",
    "            self.model.zero_grad()\n",
    "            raise e from None\n",
    "        finally:\n",
    "            if(self.learn.disc_pass):\n",
    "                self.dloss_scale=self.loss_scale\n",
    "            else:\n",
    "                self.gloss_scale=self.loss_scale\n",
    "    def after_step(self):\n",
    "        self.model.zero_grad()\n",
    "    def after_fit(self):\n",
    "        delattr(self, \"disc_model_pgs\")\n",
    "        delattr(self, \"gen_model_pgs\")\n",
    "        delattr(self, \"model_pgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SaveModelCallback(Callback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training and loads it at the end.\"\n",
    "    def __init__(self, fname='model', with_opt=True):\n",
    "        super().__init__()\n",
    "        store_attr(self, 'fname,with_opt')\n",
    "\n",
    "    def _save(self, name):\n",
    "        if(self.learn.epoch%20==0):\n",
    "            self.learn.save(name, with_opt=self.with_opt)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        self._save(f'{self.fname}_{self.epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2020-05-11-FP16-Blog_Post-2.ipynb.\n",
      "Converted 2020-05-12-GAN-Blog-Post.ipynb.\n",
      "Converted 2020-06-11-FP16-Copy1.ipynb.\n",
      "Converted 2020-06-11-FP16.ipynb.\n",
      "Converted 2020-06-11-Training-GANs-Copy1.ipynb.\n",
      "Converted 2020-06-11-Training-GANs.ipynb.\n",
      "Converted 2020-06-11-UGATIT-a-GAN-in-fp16.ipynb.\n",
      "Converted 5_17_Tutor.ipynb.\n",
      "Converted Adam Uses too much gpu memory.ipynb.\n",
      "Converted Callbacks.ipynb.\n",
      "Converted Dataloading Original.ipynb.\n",
      "Converted Dataloading-Copy1.ipynb.\n",
      "Converted Dataloading-Copy2.ipynb.\n",
      "Converted Dataloading-Copy3.ipynb.\n",
      "Converted Dataloading-Copy4.ipynb.\n",
      "Converted Dataloading-Dec31.ipynb.\n",
      "Converted Dataloading-Oct22.ipynb.\n",
      "Converted Dataloading-Split.ipynb.\n",
      "Converted Dataloading.ipynb.\n",
      "Converted Dataloading_10_14_2019.ipynb.\n",
      "Converted Kernel Inception Distance.ipynb.\n",
      "Converted Techniques for using small GPUs.ipynb.\n",
      "Converted TensorboardCallback.ipynb.\n",
      "Converted UGATIT-Dec31Backup.ipynb.\n",
      "Converted UGATIT-networks.ipynb.\n",
      "Converted UGATIT.ipynb.\n",
      "Converted UGATIT_Original.ipynb.\n",
      "Converted Untitled-Copy1.ipynb.\n",
      "Converted Untitled-Copy2.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted Untitled1.ipynb.\n",
      "Converted WandbBug.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
