{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp anime.ugatit_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464192.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512 //3 // 2 *512  //3 // 2 * 64 * 2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MixedPrecision_LayerNorm(nn.LayerNorm):\n",
    "    def __init__(self, *args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "    def forward(self, input):\n",
    "        dtype=input.dtype\n",
    "        out = super().forward(input.float())\n",
    "        return out.to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.randn([1,3,120,120])\n",
    "mln=MixedPrecision_LayerNorm(input.shape)\n",
    "ln=nn.LayerNorm(input.shape)\n",
    "assert(torch.eq(mln(input),ln(input)).all().item())\n",
    "((mln(input.half()).float()-ln(input.float())).abs()<0.01).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, use_bias):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        conv_block = []\n",
    "        conv_block += [nn.ReflectionPad2d(1),\n",
    "                       nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias),\n",
    "                       nn.InstanceNorm2d(dim),\n",
    "                       nn.ReLU(True)]\n",
    "\n",
    "        conv_block += [nn.ReflectionPad2d(1),\n",
    "                       nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias),\n",
    "                       nn.InstanceNorm2d(dim)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=6, img_size=256, light=False):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        self.n_blocks = n_blocks\n",
    "        self.img_size = img_size\n",
    "        self.light = light\n",
    "\n",
    "        DownBlock = []\n",
    "        DownBlock += [nn.ReflectionPad2d(3),\n",
    "                      nn.Conv2d(input_nc, ngf, kernel_size=7, stride=3, padding=0, bias=False),\n",
    "                      nn.InstanceNorm2d(ngf),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        # Down-Sampling\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            DownBlock += [nn.ReflectionPad2d(1),\n",
    "                          nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=0, bias=False),\n",
    "                          nn.InstanceNorm2d(ngf * mult * 2),\n",
    "                          nn.ReLU(True)]\n",
    "\n",
    "        # Down-Sampling Bottleneck\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            DownBlock += [ResnetBlock(ngf * mult, use_bias=False)]\n",
    "\n",
    "        # Class Activation Map\n",
    "        self.gap_fc = nn.Linear(ngf * mult, 1, bias=False)\n",
    "        self.gmp_fc = nn.Linear(ngf * mult, 1, bias=False)\n",
    "        self.conv1x1 = nn.Conv2d(ngf * mult * 2, ngf * mult, kernel_size=1, stride=1, bias=True)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "        # Gamma, Beta block\n",
    "        if self.light:\n",
    "            FC = [nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
    "                  nn.ReLU(True),\n",
    "                  nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
    "                  nn.ReLU(True)]\n",
    "        else:\n",
    "            #img_size //3 // mult *img_size // mult //3 * ngf * mult\n",
    "            FC = [nn.Linear( 473344 , ngf * mult, bias=False),\n",
    "                  nn.ReLU(True),\n",
    "                  MixedPrecision_LayerNorm(ngf * mult),\n",
    "                  nn.Linear(ngf * mult, ngf * mult, bias=False),\n",
    "                  nn.ReLU(True),\n",
    "                  MixedPrecision_LayerNorm(ngf * mult),\n",
    "                 ]\n",
    "        self.gamma = nn.Linear(ngf * mult, ngf * mult, bias=False)\n",
    "        self.beta = nn.Linear(ngf * mult, ngf * mult, bias=False)\n",
    "\n",
    "        # Up-Sampling Bottleneck\n",
    "        for i in range(n_blocks):\n",
    "            setattr(self, 'UpBlock1_' + str(i+1), ResnetAdaILNBlock(ngf * mult, use_bias=False))\n",
    "\n",
    "        # Up-Sampling\n",
    "        UpBlock2 = []\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            UpBlock2 += [nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                         nn.ReflectionPad2d(1),\n",
    "                         nn.Conv2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=1, padding=0, bias=False),\n",
    "                         ILN(int(ngf * mult / 2)),\n",
    "                         nn.ReLU(True)]\n",
    "\n",
    "        UpBlock2 += [ nn.Upsample(scale_factor=3, mode='nearest'),\n",
    "                     nn.ReflectionPad2d(1),\n",
    "                     nn.Conv2d(ngf, output_nc, kernel_size=7, stride=1, padding=0, bias=False),\n",
    "                     nn.Tanh()]\n",
    "\n",
    "        self.DownBlock = nn.Sequential(*DownBlock)\n",
    "        self.FC = nn.Sequential(*FC)\n",
    "        self.UpBlock2 = nn.Sequential(*UpBlock2)\n",
    "        self.out_scale = torch.tensor(2., requires_grad=False)\n",
    "        self.out_shift = torch.tensor(1., requires_grad=False)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.DownBlock(input)\n",
    "\n",
    "        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n",
    "        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))\n",
    "        gap_weight = list(self.gap_fc.parameters())[0]\n",
    "        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)\n",
    "        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))\n",
    "        gmp_weight = list(self.gmp_fc.parameters())[0]\n",
    "        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        cam_logit = torch.cat([gap_logit, gmp_logit], 1)\n",
    "        x = torch.cat([gap, gmp], 1)\n",
    "        x = self.relu(self.conv1x1(x))\n",
    "\n",
    "        heatmap = torch.sum(x, dim=1, keepdim=True)\n",
    "\n",
    "        if self.light:\n",
    "            x_ = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n",
    "            x_ = self.FC(x_.view(x_.shape[0], -1))\n",
    "        else:\n",
    "            x_ = self.FC(x.view(x.shape[0], -1))\n",
    "        gamma, beta = self.gamma(x_), self.beta(x_)\n",
    "\n",
    "\n",
    "        for i in range(self.n_blocks):\n",
    "            x = getattr(self, 'UpBlock1_' + str(i+1))(x, gamma, beta)\n",
    "        out = (self.UpBlock2(x)+self.out_shift)/self.out_scale\n",
    "\n",
    "        return out, cam_logit, heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResnetAdaILNBlock(nn.Module):\n",
    "    def __init__(self, dim, use_bias):\n",
    "        super(ResnetAdaILNBlock, self).__init__()\n",
    "        self.pad1 = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias)\n",
    "        self.norm1 = adaILN(dim)\n",
    "        self.relu1 = nn.ReLU(True)\n",
    "\n",
    "        self.pad2 = nn.ReflectionPad2d(1)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=0, bias=use_bias)\n",
    "        self.norm2 = adaILN(dim)\n",
    "\n",
    "    def forward(self, x, gamma, beta):\n",
    "        out = self.pad1(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.norm1(out, gamma, beta)\n",
    "        out = self.relu1(out)\n",
    "        out = self.pad2(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out, gamma, beta)\n",
    "\n",
    "        return out + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class adaILN(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super(adaILN, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.rho_sigmoid=nn.Sigmoid()\n",
    "        self.rho.data.fill_(0.0)\n",
    "        self.IN = nn.InstanceNorm2d(num_features,affine=False)\n",
    "        #self.LN = nn.LayerNorm(num_features,elementwise_affine=False)\n",
    "\n",
    "    def forward(self, input, gamma, beta):\n",
    "        out = self.IN(input)\n",
    "        out = self.rho_sigmoid(self.rho).expand(input.shape[0], -1, -1, -1) * out \n",
    "        out_ln = F.layer_norm(\n",
    "            input, input.shape[1:], None,None, self.eps)\n",
    "        #rho=self.rho_sigmoid(self.rho.expand(input.shape[0], -1, -1, -1))        \n",
    "        out_ln = (1-self.rho_sigmoid(self.rho).expand(input.shape[0], -1, -1, -1)) * out_ln\n",
    "        out = out + out_ln\n",
    "        out = out * gamma.unsqueeze(2).unsqueeze(3) \n",
    "        out = out + beta.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        return out.to(input.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ILN(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super(ILN, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.rho_sigmoid=nn.Sigmoid()\n",
    "        self.gamma = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.beta = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.rho.data.fill_(0.0)\n",
    "        self.gamma.data.fill_(1.0)\n",
    "        self.beta.data.fill_(0.0)\n",
    "        self.IN = nn.InstanceNorm2d(num_features,affine=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.IN(input)\n",
    "        out = self.rho_sigmoid(self.rho.expand(input.shape[0], -1, -1, -1)) * out \n",
    "        out_ln = F.layer_norm(\n",
    "            input, input.shape[1:], None,None, self.eps)\n",
    "        out_ln = (1-self.rho_sigmoid(self.rho).expand(input.shape[0], -1, -1, -1)) * out_ln\n",
    "        out = out + out_ln\n",
    "        out = out * self.gamma.expand(input.shape[0], -1, -1, -1)\n",
    "        out = out + self.beta.expand(input.shape[0], -1, -1, -1)\n",
    "\n",
    "        return out.to(input.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=5):\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = [nn.ReflectionPad2d(1),\n",
    "                 nn.utils.spectral_norm(\n",
    "                 nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=0, bias=True)),\n",
    "                 nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        for i in range(1, n_layers - 2):\n",
    "            mult = 2 ** (i - 1)\n",
    "            model += [nn.ReflectionPad2d(1),\n",
    "                      nn.utils.spectral_norm(\n",
    "                      nn.Conv2d(ndf * mult, ndf * mult * 2, kernel_size=4, stride=2, padding=0, bias=True)),\n",
    "                      nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        mult = 2 ** (n_layers - 2 - 1)\n",
    "        model += [nn.ReflectionPad2d(1),\n",
    "                  nn.utils.spectral_norm(\n",
    "                  nn.Conv2d(ndf * mult, ndf * mult * 2, kernel_size=4, stride=1, padding=0, bias=True)),\n",
    "                  nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        # Class Activation Map\n",
    "        mult = 2 ** (n_layers - 2)\n",
    "        self.gap_fc = nn.utils.spectral_norm(nn.Linear(ndf * mult, 1, bias=False))\n",
    "        self.gmp_fc = nn.utils.spectral_norm(nn.Linear(ndf * mult, 1, bias=False))\n",
    "        self.conv1x1 = nn.Conv2d(ndf * mult * 2, ndf * mult, kernel_size=1, stride=1, bias=True)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "        self.conv = nn.utils.spectral_norm(\n",
    "            nn.Conv2d(ndf * mult, 1, kernel_size=4, stride=1, padding=0, bias=False))\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.model(input)\n",
    "\n",
    "        gap = torch.nn.functional.adaptive_avg_pool2d(x, 1)\n",
    "        gap_logit = self.gap_fc(gap.view(x.shape[0], -1))\n",
    "        gap_weight = list(self.gap_fc.parameters())[0]\n",
    "        gap = x * gap_weight.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        gmp = torch.nn.functional.adaptive_max_pool2d(x, 1)\n",
    "        gmp_logit = self.gmp_fc(gmp.view(x.shape[0], -1))\n",
    "        gmp_weight = list(self.gmp_fc.parameters())[0]\n",
    "        gmp = x * gmp_weight.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        cam_logit = torch.cat([gap_logit, gmp_logit], 1)\n",
    "        x = torch.cat([gap, gmp], 1)\n",
    "        x = self.leaky_relu(self.conv1x1(x))\n",
    "\n",
    "        heatmap = torch.sum(x, dim=1, keepdim=True)\n",
    "\n",
    "        x = self.pad(x)\n",
    "        out = self.conv(x)\n",
    "\n",
    "        return out, cam_logit, heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RhoClipper(object):\n",
    "\n",
    "    def __init__(self, min, max, on=False):\n",
    "        self.clip_min = min\n",
    "        self.clip_max = max\n",
    "        self.on=on\n",
    "        assert min < max\n",
    "\n",
    "    def __call__(self, module):\n",
    "\n",
    "        if self.on and hasattr(module, 'rho'):\n",
    "            w = module.rho.data\n",
    "            w = w.clamp(self.clip_min, self.clip_max)\n",
    "            module.rho.data = w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Dataloading Original.ipynb.\n",
      "Converted Dataloading-Dec31.ipynb.\n",
      "Converted Dataloading-Oct22.ipynb.\n",
      "Converted Dataloading.ipynb.\n",
      "Converted Dataloading_10_14_2019.ipynb.\n",
      "Converted Kernel Inception Distance.ipynb.\n",
      "Converted TensorboardCallback.ipynb.\n",
      "Converted UGATIT-Dec31Backup.ipynb.\n",
      "Converted UGATIT-networks.ipynb.\n",
      "Converted UGATIT.ipynb.\n",
      "Converted UGATIT_Original.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from fast.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
